Theory is when you know everthing but nothing works.
Practice is when everything works but no one knows why.
In our lab, theory and practice are combined: nothing works and no one knows why.

- Decaying the learning rate
- Moving Averages
- tf.control_dependencies
- https://www.tensorflow.org/api_guides/python/image
- https://www.tensorflow.org/deploy/distributed
- tf on docker/mesos/k8s: https://www.tensorflow.org/deploy/distributed
- cuda, gpu
- scaffold


CONV

- slice layer
- u-net
- pascal voc/coco
- deconvnet

AlexNet -> VGG -> GooLeNet -> ResNet -> ResNeXt
R-CNN -> SPP-Net -> Fast/Faster R-CNN
YOLO -> SSD -> R-FCN
FCN -> SegNet/DeconvNet -> DeepLab
Vanilla RNN -> LSTM -> GRU
GAN -> DCGAN -> wGAN
SRGAN, SalGAN, RLA

1）Generative Adversarial Networks；2）Conditional Generative Adversarial Nets；3）Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks。

network training by reducing internal covariate shift. In ICML, 2015.
- M.D.ZeilerandR.Fergus.Visualizingandunderstandingconvolu- tional neural networks. In ECCV, 2014.
- P.Sermanet,D.Eigen,X.Zhang,M.Mathieu,R.Fergus,andY.Le- Cun. Overfeat: Integrated recognition, localization and detection using convolutional networks. In ICLR, 2014.

notorious problem of vanishing/exploding gradients [1, 9]
- [9] X. Glorot and Y. Bengio. Understanding the difficulty of training
deep feedforward neural networks. In AISTATS, 2010.
- [1] Y.Bengio,P.Simard,andP.Frasconi.Learninglong-termdependen- cies with gradient descent is difficult. IEEE Transactions on Neural Networks, 5(2):157–166, 1994.

This problem, however, has been largely addressed by normalized initialization [23, 9, 37, 13] and intermediate normalization layers [16],
- [16] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep
network training by reducing internal covariate shift. In ICML, 2015.
- [23] Y.LeCun,L.Bottou,G.B.Orr,andK.-R.Mu ̈ller.Efficientbackprop. In Neural Networks: Tricks of the Trade, pages 9–50. Springer, 1998.
- [9] X. Glorot and Y. Bengio. Understanding the difficulty of training
deep feedforward neural networks. In AISTATS, 2010.
- [37] A. M. Saxe, J. L. McClelland, and S. Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. arXiv:1312.6120, 2013.
- [13] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In
ICCV, 2015.

weight initialize
[13] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In
ICCV, 2015.

data argument
[24] C.-Y. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu. Deeply-
supervised nets. arXiv:1409.5185, 2014.
