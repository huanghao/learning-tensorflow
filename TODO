Theory is when you know everthing but nothing works.
Practice is when everything works but no one knows why.
In our lab, theory and practice are combined: nothing works and no one knows why.

- Decaying the learning rate
- Moving Averages
- tf.control_dependencies
- https://www.tensorflow.org/api_guides/python/image
- https://www.tensorflow.org/deploy/distributed
- tf on docker/mesos/k8s: https://www.tensorflow.org/deploy/distributed
- cuda, gpu
- scaffold


CONV

- slice layer
- merge layer/google inception module/google lenet/concatenation/resnet(+/-/*/max/conv)
- resnet
- u-net
- pascal voc/coco
- deconvnet

AlexNet -> VGG -> GooLeNet -> ResNet -> ResNeXt
R-CNN -> SPP-Net -> Fast/Faster R-CNN
YOLO -> SSD -> R-FCN
FCN -> SegNet/DeconvNet -> DeepLab
Vanilla RNN -> LSTM -> GRU
GAN -> DCGAN -> wGAN
SRGAN, SalGAN, RLA
