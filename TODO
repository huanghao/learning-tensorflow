Theory is when you know everthing but nothing works.
Practice is when everything works but no one knows why.
In our lab,theory and practice are  combined:nothing works and no one knows why.

- Decaying the learning rate
- Moving Averages
- tf.train.SessionRunHook
- tf.train.MonitoredTrainingSession
- tf.control_dependencies
- https://www.tensorflow.org/api_guides/python/image
- https://www.tensorflow.org/deploy/distributed
- tf on docker/mesos/k8s: https://www.tensorflow.org/deploy/distributed
- cuda, gpu
- scaffold
- global_step = tf.contrib.framework.get_or_create_global_step()


CONV

- 1x1 conv, googleNet paper
- normalization layer/local response normalization/batch normalization
- slice layer
- merge layer/google inception module/google lenet/concatenation/resnet(+/-/*/max/conv)
- vgg
- resnet
- u-net
- fcn, input size?
