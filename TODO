Theory is when you know everthing but nothing works.
Practice is when everything works but no one knows why.
In our lab, theory and practice are combined: nothing works and no one knows why.

- Decaying the learning rate
- Moving Averages
- tf.control_dependencies
- https://www.tensorflow.org/api_guides/python/image
- https://www.tensorflow.org/deploy/distributed
- tf on docker/mesos/k8s: https://www.tensorflow.org/deploy/distributed
- cuda, gpu
- scaffold
- pascal voc/coco

* AlexNet -> VGG -> GooLeNet -> ResNet -> ResNeXt
R-CNN -> SPP-Net -> Fast/Faster R-CNN
YOLO -> SSD -> R-FCN
FCN -> SegNet/DeconvNet -> DeepLab
http://www.360doc.com/content/15/0611/23/20625683_477507431.shtml
http://www.360doc.com/content/17/0223/12/39751066_631368613.shtml
Vanilla RNN -> LSTM -> GRU
GAN -> DCGAN -> wGAN
SRGAN, SalGAN, RLA
Boltzmann machine 
http://www.deeplearningbook.org/

notorious problem of vanishing/exploding gradients [1, 9]
- [9] X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks. In AISTATS, 2010.
- [1] Y.Bengio,P.Simard,andP.Frasconi.Learninglong-termdependen- cies with gradient descent is difficult. IEEE Transactions on Neural Networks, 5(2):157–166, 1994.

This problem, however, has been largely addressed by normalized initialization [23, 9, 37, 13] and intermediate normalization layers [16], network training by reducing internal covariate shift. In ICML, 2015.
- [23] Y.LeCun,L.Bottou,G.B.Orr,andK.-R.Mu ̈ller.Efficientbackprop. In Neural Networks: Tricks of the Trade, pages 9–50. Springer, 1998.
- [9] X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks. In AISTATS, 2010.
- [37] A. M. Saxe, J. L. McClelland, and S. Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. arXiv:1312.6120, 2013.
- [13] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In ICCV, 2015.  
weight initialize
[13] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In ICCV, 2015.

data argument
[24] C.-Y. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu. Deeply supervised nets. arXiv:1409.5185, 2014.

- [16] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep
https://r2rt.com/implementing-batch-normalization-in-tensorflow.html
http://ruishu.io/2016/12/27/batchnorm/

dropout: [10] G.E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R.R. Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580, 2012.

[20] V. Nair and G. E. Hinton. Rectified linear units improve restricted boltzmann machines. In Proc. 27th International Conference on Machine Learning, 2010.

caffe
https://docs.google.com/presentation/d/1UeKXVgRvvxg9OUdh_UiC5G71UMscNPlvArsWER41PsU/edit#slide=id.p
https://docs.google.com/presentation/d/1HxGdeq8MPktHaPb-rlmYYQ723iWzq9ur6Gjo71YiG0Y/edit#slide=id.g105265477c_0_184
Caffe Model Zoo

生成评论：rnn，gan
生成验证码
识别验证码
safetyNet aatestation api
文案检查，错别字，漏字

