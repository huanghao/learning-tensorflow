{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing and Understanding Convolutional Networks\n",
    "----\n",
    "\n",
    "arXiv: 1311.2901v3 [cs.CV] 28 Nov 2013"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abstract\n",
    "\n",
    "Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark (Krizhevsky et al., 2012). However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architec- tures that outperform Krizhevsky et al. on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.\n",
    "\n",
    "大型的卷积网络最近在ImageNet上显示很强的分类能力。但是对于为什么它们能够表现如此出色，或者如何改进并不是很清楚。这篇文章解决了这两个问题。我们引进了一种新颖的视觉技术，对中间层的属性和分类器的操作给予一些直观的洞悉。用来作为诊断的方法，我们找到了一种更好的结构比Krizhevskey2012更优秀。我们还进行了一些移除的试验来揭示不同的层对于性能的贡献。我们的模型在别的数据集上泛化很好：对最后的softmax分类器重新训练，我们的模型打败了在Caltech-101和Caltech-256上最前沿的技术。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 Introduction\n",
    "\n",
    "Since their introduction by (LeCun et al., 1989) in the early 1990’s, Convolutional Networks (convnets) have demonstrated excellent performance at tasks such as hand-written digit classification and face detec- tion. In the last year, several papers have shown that they can also deliver outstanding performance on more challenging visual classification tasks. (Ciresan et al., 2012) demonstrate state-of-the-art performance on NORB and CIFAR-10 datasets. Most notably, (Krizhevsky et al., 2012) show record beating perfor- mance on the ImageNet 2012 classification benchmark, with their convnet model achieving an error rate of 16.4%, compared to the 2nd place result of 26.1%. Several factors are responsible for this renewed inter-est in convnet models: (i) the availability of much larger training sets, with millions of labeled exam- ples; (ii) powerful GPU implementations, making the training of very large models practical and (iii) bet- ter model regularization strategies, such as Dropout (Hinton et al., 2012).\n",
    "\n",
    "自从convnets在90年代初被LeCun1989引入，在手写数字分类和人脸识别上表现出了很好的效果。去年，几篇论文也显示了它们在更有挑战视觉分类任务上也可以达到很好的性能。（Ciresan2012）在NORB和CIFAR-10上达到了最高的水平。最有名的，Krizhevsky2012在ImageNet2012分类指标上做到非常好的效果，错误率达到16.4%，比第二名好了26.1%。在这个重新被引起兴趣的convnet模型上有几个因素：(i)大型数据集的出现，包含了百万计的标签样本。(ii)计算能力更强的GPU使得模型可以变得更大。(iii)更好的模型正规化测试，例如dropout（Hinton2012）\n",
    "\n",
    "Despite this encouraging progress, there is still lit- tle insight into the internal operation and behavior of these complex models, or how they achieve such good performance. From a scientific standpoint, this is deeply unsatisfactory. Without clear understanding of how and why they work, the development of better models is reduced to trial-and-error. In this paper we introduce a visualization technique that reveals the in- put stimuli that excite individual feature maps at any layer in the model. It also allows us to observe the evolution of features during training and to diagnose potential problems with the model. The visualization technique we propose uses a multi-layered Deconvo- lutional Network (deconvnet), as proposed by (Zeiler et al., 2011), to project the feature activations back to the input pixel space. We also perform a sensitivity analysis of the classifier output by occluding portions of the input image, revealing which parts of the scene are important for classification.\n",
    "\n",
    "尽管这些进展非常鼓舞人心，但是对于内部操作和这种复杂的模型的理解，还要他们如何达到这么好的性能的理解还是很少。从科学的立场来看，这非常低不能接受。如果不能很好地理解它们是如何工作的，要开发出更好的模型只是试错和不断的试验。这篇文章，我们提出一种视觉方法可以揭示输入刺激某层的某些特征图。这种视觉方法使用多层的反卷积网络（deconvnet），由Zeiler2011提出。用来把特征图反向生成输入像素空间。我们还做了敏感分析，来发现输入图片的哪些部分对于分类是重要的。\n",
    "\n",
    "Using these tools, we start with the architecture of (Krizhevsky et al., 2012) and explore different archi- tectures, discovering ones that outperform their results on ImageNet. We then explore the generalization abil- ity of the model to other datasets, just retraining the softmax classifier on top. As such, this is a form of su- pervised pre-training, which contrasts with the unsu- pervised pre-training methods popularized by (Hinton et al., 2006) and others (Bengio et al., 2007; Vincent et al., 2008). The generalization ability of convnet fea- tures is also explored in concurrent work by (Donahue et al., 2013).\n",
    "\n",
    "使用这些工具，我们从Krizhevsky2012的结构开始，然后探索了不同的结构，发现了能够在ImageNet做到更好的结构。然后我们把模型泛化到其他数据集上，只是重新训练了最后的softmax分类器。这是一种形式的监督预训练，相对于非监督预训练方法（Hinton2006，Bengio2007，Vincent2008）。convnet的泛化能力也在Donahue2013里被研究。\n",
    "\n",
    "1.1. Related Work\n",
    "\n",
    "Visualizing features to gain intuition about the net- work is common practice, but mostly limited to the 1st layer where projections to pixel space are possible. In higher layers this is not the case, and there are limited methods for interpreting activity. (Erhan et al., 2009) find the optimal stimulus for each unit by perform- ing gradient descent in image space to maximize the unit’s activation. This requires a careful initialization and does not give any information about the unit’s in- variances. Motivated by the latter’s short-coming, (Le et al., 2010) (extending an idea by (Berkes & Wiskott, 2006)) show how the Hessian of a given unit may be computed numerically around the optimal response, giving some insight into invariances. The problem is that for higher layers, the invariances are extremely complex so are poorly captured by a simple quadratic approximation. Our approach, by contrast, provides a non-parametric view of invariance, showing which pat- terns from the training set activate the feature map. (Donahue et al., 2013) show visualizations that iden- tify patches within a dataset that are responsible for strong activations at higher layers in the model. Our visualizations differ in that they are not just crops of input images, but rather top-down projections that reveal structures within each patch that stimulate a particular feature map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 Approach\n",
    "\n",
    "We use standard fully supervised convnet models\n",
    "throughout the paper, as defined by (LeCun et al.,\n",
    "1989) and (Krizhevsky et al., 2012). These models\n",
    "map a color 2D input image xi, via a series of lay-\n",
    "ers, to a probability vector yˆ over the C different i\n",
    "classes. Each layer consists of (i) convolution of the previous layer output (or, in the case of the 1st layer, the input image) with a set of learned filters; (ii) pass- ing the responses through a rectified linear function (relu(x) = max(x,0)); (iii) [optionally] max pooling over local neighborhoods and (iv) [optionally] a lo- cal contrast operation that normalizes the responses across feature maps. For more details of these opera- tions, see (Krizhevsky et al., 2012) and (Jarrett et al., 2009). The top few layers of the network are conven- tional fully-connected networks and the final layer is a softmax classifier. Fig. 3 shows the model used in many of our experiments.\n",
    "\n",
    "We train these models using a large set of N labeled images {x,y}, where label yi is a discrete variable indicating the true class. A cross-entropy loss func- tion, suitable for image classification, is used to com-\n",
    "pare yˆ and y . The parameters of the network (fil-ters in the convolutional layers, weight matrices in the fully-connected layers and biases) are trained by back- propagating the derivative of the loss with respect to the parameters throughout the network, and updating the parameters via stochastic gradient descent. Full details of training are given in Section 3.\n",
    "\n",
    "\n",
    "2.1. Visualization with a Deconvnet\n",
    "Understanding the operation of a convnet requires in- terpreting the feature activity in intermediate layers. We present a novel way to map these activities back to the input pixel space, showing what input pattern orig- inally caused a given activation in the feature maps. We perform this mapping with a Deconvolutional Net- work (deconvnet) (Zeiler et al., 2011). A deconvnet can be thought of as a convnet model that uses the same components (filtering, pooling) but in reverse, so instead of mapping pixels to features does the oppo- site. In (Zeiler et al., 2011), deconvnets were proposed as a way of performing unsupervised learning. Here, they are not used in any learning capacity, just as a probe of an already trained convnet.\n",
    "\n",
    "\n",
    "To examine a convnet, a deconvnet is attached to each of its layers, as illustrated in Fig. 1(top), providing a continuous path back to image pixels. To start, an input image is presented to the convnet and features computed throughout the layers. To examine a given convnet activation, we set all other activations in the layer to zero and pass the feature maps as input to the attached deconvnet layer. Then we successively (i) unpool, (ii) rectify and (iii) filter to reconstruct the activity in the layer beneath that gave rise to the chosen activation. This is then repeated until input pixel space is reached.\n",
    "\n",
    "Unpooling: In the convnet, the max pooling opera- tion is non-invertible, however we can obtain an ap- proximate inverse by recording the locations of the maxima within each pooling region in a set of switch variables. In the deconvnet, the unpooling operation uses these switches to place the reconstructions from the layer above into appropriate locations, preserving the structure of the stimulus. See Fig. 1(bottom) for an illustration of the procedure.\n",
    "Rectification: The convnet uses relu non-linearities, which rectify the feature maps thus ensuring the fea- ture maps are always positive. To obtain valid fea- ture reconstructions at each layer (which also should be positive), we pass the reconstructed signal through a relu non-linearity.\n",
    "Filtering: The convnet uses learned filters to con- volve the feature maps from the previous layer. To invert this, the deconvnet uses transposed versions of the same filters, but applied to the rectified maps, not the output of the layer beneath. In practice this means flipping each filter vertically and horizontally.\n",
    "Projecting down from higher layers uses the switch settings generated by the max pooling in the convnet on the way up. As these switch settings are peculiar to a given input image, the reconstruction obtained from a single activation thus resembles a small piece of the original input image, with structures weighted according to their contribution toward to the feature activation. Since the model is trained discriminatively, they implicitly show which parts of the input image are discriminative. Note that these projections are not samples from the model, since there is no generative process involved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
