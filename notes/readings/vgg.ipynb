{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very deep convolution networks for large-scale image recognition\n",
    "====\n",
    "\n",
    "10 Apr 2015\n",
    "\n",
    "https://arxiv.org/pdf/1409.1556.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 CONVNET CONFIGURATIONS\n",
    "\n",
    "To measure the improvement brought by the increased ConvNet depth in a fair setting, all our ConvNet layer configurations are designed using the same principles, inspired by Ciresan et al. (2011); Krizhevsky et al. (2012). In this section, we first describe a generic layout of our ConvNet configurations (Sect. 2.1) and then detail the specific configurations used in the evaluation (Sect. 2.2). Our design choices are then discussed and compared to the prior art in Sect. 2.3.\n",
    "\n",
    "为了公平地测量通过增加深度带来的改进，我们所有的层都是用相同的设计原则。在这一章，我们首先简要介绍整个网络的布局（2.1)，然后再深入到具体的求值细节（2.2）。最后在2.3来讨论和比较我们的设计原则。\n",
    "\n",
    "2.1 ARCHITECTURE\n",
    "\n",
    "During training, the input to our ConvNets is a fixed-size 224 × 224 RGB image. The only pre-processing we do is subtracting the mean RGB value, computed on the training set, from each pixel. The image is passed through a stack of convolutional (conv.) layers, where we use filters with a very small receptive field: 3 × 3 (which is the smallest size to capture the notion of left/right, up/down, center). In one of the configurations we also utilise 1 × 1 convolution filters, which can be seen as a linear transformation of the input channels (followed by non-linearity). The convolution stride is fixed to 1 pixel; the spatial padding of conv. layer input is such that the spatial resolution is preserved after convolution, i.e. the padding is 1 pixel for 3 × 3 conv. layers. Spatial pooling is carried out by five max-pooling layers, which follow some of the conv. layers (not all the conv. layers are followed by max-pooling). Max-pooling is performed over a 2 × 2 pixel window, with stride 2.\n",
    "\n",
    "在整个训练中，我们使用的输入是固定的224 x 224大小的RGB图片。唯一的预处理操作只有每个图片都减去了整个训练集的均值。图片经过一组卷积层的处理。我们使用很小感知域的卷积核：3x3（这是最小的能捕捉左右，上下和中间各方向的大小了）。在其中的一个配置里，我们还使用了1x1的卷积核，这可以被看成是对输入通道的一个线性变换（后跟非线性）。卷积的步长固定为1个像素；并使用了空间的填充来保持空间大小在卷积操作以后不变，例如，对3x3卷积使用填充1。跟在卷积层组后面的是5个空间池化层。最大池使用2x2的像素框，并且步长为2.\n",
    "\n",
    "A stack of convolutional layers (which has a different depth in different architectures) is followed by three Fully-Connected (FC) layers: the first two have 4096 channels each, the third performs 1000-way ILSVRC classification and thus contains 1000 channels (one for each class). The final layer is the soft-max layer. The configuration of the fully connected layers is the same in all networks.\n",
    "\n",
    "在成组的卷积层（不同深度）后面是3个全连接层。前两个分别有4096个通道，第三个有1000个通道，用来对应ILSVRC的1000个分类，每个通道对一个分类。最后一层是一个softmax层。fc层的配置在所有网络中都是一样的。\n",
    "\n",
    "All hidden layers are equipped with the rectification (ReLU (Krizhevsky et al., 2012)) non-linearity. We note that none of our networks (except for one) contain Local Response Normalisation (LRN) normalisation (Krizhevsky et al., 2012): as will be shown in Sect. 4, such normalisation does not improve the performance on the ILSVRC dataset, but leads to increased memory consumption and computation time. Where applicable, the parameters for the LRN layer are those of (Krizhevsky et al., 2012).\n",
    "\n",
    "所有的隐藏层都使用relu作为非线性激励。所有的网络（除了一个）都没有使用LRN（局部响应归一化），在第四章中可以看到LRN在ILSVRC数据集上并不能提升性能，反而还增加了内存和计算时间的消耗。这句不知道怎么翻译。。。\n",
    "\n",
    "2.2 CONFIGURATIONS\n",
    "\n",
    "The ConvNet configurations, evaluated in this paper, are outlined in Table 1, one per column. In the following we will refer to the nets by their names (A–E). All configurations follow the generic design presented in Sect. 2.1, and differ only in the depth: from 11 weight layers in the network A (8 conv. and 3 FC layers) to 19 weight layers in the network E (16 conv. and 3 FC layers). The width of conv. layers (the number of channels) is rather small, starting from 64 in the first layer and then increasing by a factor of 2 after each max-pooling layer, until it reaches 512.\n",
    "\n",
    "本文使用的所有卷积网络的配置列在了表1中，每一列一个网络。后面我们会使用A-E来指代这些网络。所有的配置都遵守在2.1节提到的设计原则，但在深度上有区别：从网络A的11层（8个conv，3个fc）到网络E的19层（16个conv，3个卷积）。卷积层的宽度（通道数）是很小的，从第一组的64开始增长，每通过一个最大池层以后增大一倍，直到变成512.\n",
    "\n",
    "In Table 2 we report the number of parameters for each configuration. In spite of a large depth, the number of weights in our nets is not greater than the number of weights in a more shallow net with larger conv. layer widths and receptive fields (144M weights in (Sermanet et al., 2014)).\n",
    "\n",
    "表2列出了每种配置拥有的参数数量。尽管深度比较深，但是我们网络的参数数量比起一些有较大卷积宽度和感知域的浅网络还是要少的（144兆参数Sermanet et al., 2014）\n",
    "\n",
    "2.3 DISCUSSION\n",
    "\n",
    "Our ConvNet configurations are quite different from the ones used in the top-performing entries of the ILSVRC-2012 (Krizhevsky et al., 2012) and ILSVRC-2013 competitions (Zeiler & Fergus, 2013; Sermanet et al., 2014). Rather than using relatively large receptive fields in the first conv. layers (e.g. 11 × 11 with stride 4 in (Krizhevsky et al., 2012), or 7 × 7 with stride 2 in (Zeiler & Fergus, 2013; Sermanet et al., 2014)), we use very small 3 × 3 receptive fields throughout the whole net, which are convolved with the input at every pixel (with stride 1). It is easy to see that a stack of two 3 × 3 conv. layers (without spatial pooling in between) has an effective receptive field of 5 × 5; three such layers have a 7 × 7 effective receptive field. So what have we gained by using, for instance, a stack of three 3 × 3 conv. layers instead of a single 7 × 7 layer? First, we incorporate three non-linear rectification layers instead of a single one, which makes the decision function more discriminative. Second, we decrease the number of parameters: assuming that both the input and the output of a three-layer 3 × 3 convolution stack has C channels, the stack is parametrised by 3  32C2  = 27C2 weights; at the same time, a single 7 × 7 conv. layer would require 72C2 = 49C2 parameters, i.e. 81% more. This can be seen as imposing a regularisation on the 7 × 7 conv. filters, forcing them to have a decomposition through the 3 × 3 filters (with non-linearity injected in between).\n",
    "\n",
    "我们的网络配置和那些在ILSVRC-2012，2013中表现出色的网络非常不同。他们在第一个卷积层使用较大的感知域（11x11-s4，11x11-s2），而我们在整个网络都使用很小的3x3的感知域，并且对每个像素做卷积（步长为1）。很容易看到，把两个3x3的卷积层叠起来（中间不做空间上的池化）相当于5x5的感知域；而3个3x3相当于7x7的感知域。那么我们用连续的3个3x3卷积而不是一个7x7的卷积层所得到的好处有什么呢？第一，我们得到了3个非线性的relu层而不是1个，这使得决策函数有更大的分辨能力。第二，降低了参数的数量：假设3个3x3卷积组的输入和输出都有C个通道，那一共有$3(3^2C^2) = 27C^2$个参数；而7x7的卷积层需要$7^2C^2 = 49C^2$个参数，多了81%。这可以看做对7x7的卷积滤波器增加了正规化，迫使它分解成3x3的滤波器（附带插入中间的非线性层）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 1: ConvNet configurations (shown in columns). The depth of the configurations increases from the left (A) to the right (E), as more layers are added (the added layers are shown in bold). The convolutional layer parameters are denoted as “conv⟨receptive field size⟩-⟨number of channels⟩”. The ReLU activation function is not shown for brevity.\n",
    "\n",
    "表1：卷积网络配置（以列显示）。从左（A）到右（E）深度逐渐增加，更多层被加入（新增的层显示为粗体）。conv层的参数表示为“conv<感知域大小>-通道数”。为简单起见，relu没有显示。\n",
    "\n",
    "![vgg-convnet-configuration](vgg-convnet-configuration.png)\n",
    "\n",
    "Table 2: Number of parameters (in millions).\n",
    "\n",
    "表2：参数数量（单位百万）\n",
    "\n",
    "![vgg-number-of-parameters](vgg-number-of-parameters.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The incorporation of 1 × 1 conv. layers (configuration C, Table 1) is a way to increase the non-linearity of the decision function without affecting the receptive fields of the conv. layers. Even though in our case the 1 × 1 convolution is essentially a linear projection onto the space of the same dimensionality (the number of input and output channels is the same), an additional non-linearity is introduced by the rectification function. It should be noted that 1 × 1 conv. layers have recently been utilised in the “Network in Network” architecture of Lin et al. (2014).\n",
    "\n",
    "在网络C中使用了1x1卷积层，这是一种在不改变感知域的情况下增加非线性的方法。尽管在我们这种情况下1x1卷积本质上只是一种在相同维度空间的线性变换（输入和输出的通道数一样），但是多增加了一个relu。需要指出的是，1x1卷积最近被“Network in Network”这种结构所使用。\n",
    "\n",
    "Small-size convolution filters have been previously used by Ciresan et al. (2011), but their nets are significantly less deep than ours, and they did not evaluate on the large-scale ILSVRC dataset. Goodfellow et al. (2014) applied deep ConvNets (11 weight layers) to the task of street number recognition, and showed that the increased depth led to better performance. GoogLeNet (Szegedy et al., 2014), a top-performing entry of the ILSVRC-2014 classification task, was developed independently of our work, but is similar in that it is based on very deep ConvNets (22 weight layers) and small convolution filters (apart from 3 × 3, they also use 1 × 1 and 5 × 5 convolutions). Their network topology is, however, more complex than ours, and the spatial resolution of the feature maps is reduced more aggressively in the first layers to decrease the amount of computation. As will be shown in Sect. 4.5, our model is outperforming that of Szegedy et al. (2014) in terms of the single-network classification accuracy.\n",
    "\n",
    "小尺寸的卷积核被Ciresan2011使用过，但是他们的网络比我们的要浅多了，而且他们也没有在ILSVRC这样大的数据集上评估过。Goodfellow2014使用过11层的卷积网络做过街景数字识别，并且表明了增加网络可以提高性能。GoogLeNet2014，ILSVRC-2014年分类任务的优胜者，独立研究也得到类似的结论，使用深度网络（22层）和小卷积（除了3x3，他们还使用了1x1和5x5的卷积）。但是他们的网络结构比我们的要复杂很多，而且在第一层很激进低降低了特征图的空间分辨率来减少计算。在4.5节中降维看到，我们的模型在单一网络的分类准确率上胜过Szegedy2014。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 CLASSIFICATION FRAMEWORK\n",
    "\n",
    "In the previous section we presented the details of our network configurations. In this section, we describe the details of classification ConvNet training and evaluation.\n",
    "\n",
    "前一章我们详细介绍了网络的配置。这章，我们要详细解释卷积分类的训练和求值。\n",
    "\n",
    "3.1 TRAINING\n",
    "\n",
    "The ConvNet training procedure generally follows Krizhevsky et al. (2012) (except for sampling the input crops from multi-scale training images, as explained later). Namely, the training is carried out by optimising the multinomial logistic regression objective using mini-batch gradient descent (based on back-propagation (LeCun et al., 1989)) with momentum. The batch size was set to 256, momentum to 0.9. The training was regularised by weight decay (the L2 penalty multiplier set to 5 · 10−4) and dropout regularisation for the first two fully-connected layers (dropout ratio set to 0.5). The learning rate was initially set to 10−2, and then decreased by a factor of 10 when the validation set accuracy stopped improving. In total, the learning rate was decreased 3 times, and the learning was stopped after 370K iterations (74 epochs). We conjecture that in spite of the larger number of parameters and the greater depth of our nets compared to (Krizhevsky et al., 2012), the nets required less epochs to converge due to (a) implicit regularisation imposed by greater depth and smaller conv. filter sizes; (b) pre-initialisation of certain layers.\n",
    "\n",
    "卷积网络的训练依据Krizhevsky2012（除了多尺度采样，后面会介绍）。即训练过程就是通过批梯度下降去最小化多项式逻辑回归的目标函数。（基于动量的反向传递LeCun1989）。批大小为256，动量为0.9。训练使用了权重衰减正则化（$L_2$乘法系数为$5\\times10^{-4}$）。在前两个fc层后使用了dropout层（丢弃比率为0.5）。学习率初始化为$10^{-2}$，当在验证集上的正确率不再升高时，以10倍的速度减小。一共学习率减少了3次，最后经过了370K次迭代（74个周期）。虽然相比于Krizhevskey2012我们有更多的参数和更深的结构，但是我们需要更少的收敛周期。我们推测的原因有：（a）通过更深和更小的卷积核隐含的正则化。（b）某些层的预初始化。\n",
    "\n",
    "The initialisation of the network weights is important, since bad initialisation can stall learning due to the instability of gradient in deep nets. To circumvent this problem, we began with training the configuration A (Table 1), shallow enough to be trained with random initialisation. Then, when training deeper architectures, we initialised the first four convolutional layers and the last three fully-connected layers with the layers of net A (the intermediate layers were initialised randomly). We did not decrease the learning rate for the pre-initialised layers, allowing them to change during learning. For random initialisation (where applicable), we sampled the weights from a normal distribution with the zero mean and 10−2 variance. The biases were initialised with zero. It is worth noting that after the paper submission we found that it is possible to initialise the weights without pre-training by using the random initialisation procedure of Glorot & Bengio (2010).\n",
    "\n",
    "权重的初始化很重要，因为不好的初始化会导致梯度不稳定而让学习停滞。为了解决这个问题，我们从网络A开始训练，它浅到可以使用随机初始化。然后用网络A的前4个conv层和后3个fc层的参数，来训练更深的网络，中间的层随机初始化。对预初始化的层没有减低学习率，让他们在继续训练。对于随机初始化，我们使用均值为0，方差为$10^{-2}$的正态分布。偏移初始化为0。值得一提的是，在论文提交之后，我们发现通过使用Glorot&Bengio2010的随机初始化方法，而不用预训练，也是可能的。\n",
    "\n",
    "To obtain the fixed-size 224×224 ConvNet input images, they were randomly cropped from rescaled training images (one crop per image per SGD iteration). To further augment the training set, the crops underwent random horizontal flipping and random RGB colour shift (Krizhevsky et al., 2012). Training image rescaling is explained below.\n",
    "\n",
    "为了得到224x224的输入图片，他们从拉伸了尺度的训练图片中随机截取出来（每一次SGD迭代在每个图片上随机截取一次）。为了增大训练集，截取后的图片又随机进行了水平翻转和随机RGB颜色的转换。图片尺度的变换如下。\n",
    "\n",
    "Training image size. Let S be the smallest side of an isotropically-rescaled training image, from which the ConvNet input is cropped (we also refer to S as the training scale). While the crop size is fixed to 224 × 224, in principle S can take on any value not less than 224: for S = 224 the crop will capture whole-image statistics, completely spanning the smallest side of a training image; for S ≫ 224 the crop will correspond to a small part of the image, containing a small object or an object part.\n",
    "\n",
    "训练图片大小。设`S`为各向同性拉伸后的训练图片的最小边的大小。（是不是指保持长宽比拉伸，然后短的边是S）。卷积网络的输入是从这个拉伸后的图片中截取的（我们也称S为训练尺度）。截取尺寸固定为224 x 224，理论上S可以是大于等于224的任何数值：当S=224时，截取包含了整个图片的统计，完整的跨越了训练图片最小的一边。当S >> 224时，截取只对应图片中很小的一个部分，包含了一个小的物体或者物体的一部分。\n",
    "\n",
    "We consider two approaches for setting the training scale S. The first is to fix S, which corresponds to single-scale training (note that image content within the sampled crops can still represent multi-scale image statistics). In our experiments, we evaluated models trained at two fixed scales: S = 256 (which has been widely used in the prior art (Krizhevsky et al., 2012; Zeiler & Fergus, 2013; Sermanet et al., 2014)) and S = 384. Given a ConvNet configuration, we first trained the network using S = 256. To speed-up training of the S = 384 network, it was initialised with the weights pre-trained with S = 256, and we used a smaller initial learning rate of 10−3.\n",
    "\n",
    "我们使用两个方法来设置训练尺度S。第一种为固定的S，这对应于单尺度训练。（注意图片内容在截取后的图片仍能表达多尺度的图片信息）。在我们的试验中，我们在两个固定的尺度上进行了评估：S=256（第一种被最前沿的研究广泛使用）和S=384。在跟定了网络配置后，我们先用S=256来训练。为提升S=384网络训练的速度，参数会用S=256的来初始化，然后使用$10^{-3}$作为初始的学习率。\n",
    "\n",
    "The second approach to setting S is multi-scale training, where each training image is individually rescaled by randomly sampling S from a certain range [Smin,Smax] (we used Smin = 256 and Smax = 512). Since objects in images can be of different size, it is beneficial to take this into account during training. This can also be seen as training set augmentation by scale jittering, where a single model is trained to recognise objects over a wide range of scales. For speed reasons, we trained multi-scale models by fine-tuning all layers of a single-scale model with the same configuration, pre-trained with fixed S = 384.\n",
    "\n",
    "第二种方法是设置S为多尺度训练。每个训练图片独立低随机地被拉伸到一系列的S。$[S_{min}, S_{max}]$ (我们设$S_{min} = 256$，$S_{max} = 512$)。因为图片中的物体可能有不同的大小，在训练的时候把这个因素考虑进去也是有意的。这可以看成对训练集在尺度上做增强，模型被训练用来识别一定范围大小的物体。因为速度的原因，我们在训练多尺度模型的时候，使用已经训练的单尺度S=384已经得到的参数来初始化。\n",
    "\n",
    "3.2 TESTING\n",
    "\n",
    "At test time, given a trained ConvNet and an input image, it is classified in the following way. First, it is isotropically rescaled to a pre-defined smallest image side, denoted as Q (we also refer to it as the test scale). We note that Q is not necessarily equal to the training scale S (as we will show in Sect. 4, using several values of Q for each S leads to improved performance). Then, the network is applied densely over the rescaled test image in a way similar to (Sermanet et al., 2014). Namely, the fully-connected layers are first converted to convolutional layers (the first FC layer to a 7 × 7 conv. layer, the last two FC layers to 1 × 1 conv. layers). The resulting fully-convolutional net is then applied to the whole (uncropped) image. The result is a class score map with the number of channels equal to the number of classes, and a variable spatial resolution, dependent on the input image size. \n",
    "\n",
    "在测试时，给定一个已经训练好的网络和一个输入图片，分类的过程如下。首先，各向同性拉伸到预定的最小边大小，成为`Q`（也称为测试尺度）。测试尺度`Q`不一定要和训练尺度`S`一样大（在第4章可以看到，对每个S使用不同的Q可以提升性能）。然后，fc层先转换成conv层（第一个fc变成7x7的conv，后两个fc变成1x1的conv层）。结果形成的全卷积网络（FCN）应用到整个图片上（截取之前）。结果是一个分类评分的映射。通道数等于分类数。根据输入图片的大小，输出空间的大小也是变化的。\n",
    "\n",
    "Finally, to obtain a fixed-size vector of class scores for the image, the class score map is spatially averaged (sum-pooled). We also augment the test set by horizontal flipping of the images; the soft-max class posteriors of the original and flipped images are averaged to obtain the final scores for the image.\n",
    "\n",
    "最后，为了得到一个固定大小的向量来表示图片分类评分，分类评分映射在空间上做平均。我们同样适用水平翻转来进行数据增强。翻转以后和之前的softmax平均之后得到图片的最后得分。\n",
    "\n",
    "Since the fully-convolutional network is applied over the whole image, there is no need to sample multiple crops at test time (Krizhevsky et al., 2012), which is less efficient as it requires network re-computation for each crop. At the same time, using a large set of crops, as done by Szegedy et al. (2014), can lead to improved accuracy, as it results in a finer sampling of the input image compared to the fully-convolutional net. Also, multi-crop evaluation is complementary to dense evaluation due to different convolution boundary conditions: when applying a ConvNet to a crop, the convolved feature maps are padded with zeros, while in the case of dense evaluation the padding for the same crop naturally comes from the neighbouring parts of an image (due to both the convolutions and spatial pooling), which substantially increases the overall network receptive field, so more context is captured. While we believe that in practice the increased computation time of multiple crops does not justify the potential gains in accuracy, for reference we also evaluate our networks using 50 crops per scale (5 × 5 regular grid with 2 flips), for a total of 150 crops over 3 scales, which is comparable to 144 crops over 4 scales used by Szegedy et al. (2014).\n",
    "\n",
    "因为全卷积网络是应用到整张图片上的，所以没有必要在测试的时候再多次截取采样了，多次计算比较耗时。使用很多的截取可以提醒准确度。而且，多次截取是对dense eval的一种补充，因为不同的卷积边界条件：当应用一个ConvNet到一个截取时，卷积特征图用0填充。用dense eval时填充自然来自图片的邻居。\n",
    "\n",
    "3.3 IMPLEMENTATION DETAILS\n",
    "\n",
    "Our implementation is derived from the publicly available C++ Caffe toolbox (Jia, 2013) (branched out in December 2013), but contains a number of significant modifications, allowing us to perform training and evaluation on multiple GPUs installed in a single system, as well as train and evaluate on full-size (uncropped) images at multiple scales (as described above). Multi-GPU training exploits data parallelism, and is carried out by splitting each batch of training images into several GPU batches, processed in parallel on each GPU. After the GPU batch gradients are computed, they are averaged to obtain the gradient of the full batch. Gradient computation is synchronous across the GPUs, so the result is exactly the same as when training on a single GPU.\n",
    "\n",
    "我们的实现基于公开的C++ Caffe工具箱（2013年12月分支出来），但是包含了一系列重要的修改，使得我们可以在多个gpu上和全尺寸图片的多个尺寸上进行训练和求值。多gpu训练采用数据并行的方式，通过把一批里的图片切分到多个gpu上，并行计算。在批的梯度计算完成之后，计算他们的平均值来得到这个批的梯度。梯度计算在多个gpu上是同步进行的，所以结果和我们在单个gpu上得到的结果是一致的。\n",
    "\n",
    "While more sophisticated methods of speeding up ConvNet training have been recently proposed (Krizhevsky, 2014), which employ model and data parallelism for different layers of the net, we have found that our conceptually much simpler scheme already provides a speedup of 3.75 times on an off-the-shelf 4-GPU system, as compared to using a single GPU. On a system equipped with four NVIDIA Titan Black GPUs, training a single net took 2–3 weeks depending on the architecture.\n",
    "\n",
    "最近一些更高级的提升卷积网络训练速度的方法已经被提出了Krizhevsky2014。这个方法通过对不同层的模型和数据并行计算。我们发现虽然我们的方法概念上更简单，但在4-gpu的系统上相对于单gpu系统已经得到了3.75倍的提速。在一个配置了4个NVIDIA Titan Black GPUs的系统上，训练单个网络花费了2-3周。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 CLASSIFICATION EXPERIMENTS\n",
    "\n",
    "Dataset. In this section, we present the image classification results achieved by the described ConvNet architectures on the ILSVRC-2012 dataset (which was used for ILSVRC 2012–2014 challenges). The dataset includes images of 1000 classes, and is split into three sets: training (1.3M images), validation (50K images), and testing (100K images with held-out class labels). The classification performance is evaluated using two measures: the top-1 and top-5 error. The former is a multi-class classification error, i.e. the proportion of incorrectly classified images; the latter is the main evaluation criterion used in ILSVRC, and is computed as the proportion of images such that the ground-truth category is outside the top-5 predicted categories.\n",
    "\n",
    "数据集。这一章，我们给出在ILSVRC-2012数据集上做图片分类的结果。（这个数据集在2012-2014被作为竞赛的数据使用）。这个数据集的图片包含了1000个分类。被分成三个集合，训练集（130w张图片），验证集（5w张），测试集（10w张，且分类标签未给出）。分类性能由两个数据评估：top-1和top-5错误率。前一个是多分类的分类错误率，也就是说，错误分类的图片的比率。\n",
    "\n",
    "For the majority of experiments, we used the validation set as the test set. Certain experiments were also carried out on the test set and submitted to the official ILSVRC server as a “VGG” team entry to the ILSVRC-2014 competition (Russakovsky et al., 2014).\n",
    "\n",
    "在大部分的试验中，我们把验证集当成测试集。\n",
    "\n",
    "4.1 SINGLE SCALE EVALUATION\n",
    "\n",
    "We begin with evaluating the performance of individual ConvNet models at a single scale with the layer configurations described in Sect. 2.2. The test image size was set as follows: Q = S for fixed S, and Q = 0.5(Smin + Smax) for jittered S ∈ [Smin, Smax]. The results of are shown in Table 3.\n",
    "\n",
    "我们开始评估单个conv网路在单尺度变换下的性能。测试图片的大小设置为Q = S当S为固定大小时。$Q = 0.5 (S_{min} + S_{max})$当S属于$[S_{min}, S_{max}]$之间时。结果显示在表3。\n",
    "\n",
    "First, we note that using local response normalisation (A-LRN network) does not improve on the model A without any normalisation layers. We thus do not employ normalisation in the deeper architectures (B–E).\n",
    "\n",
    "首先，我们发现使用局部响应归一化（网络A-LRN）不能提升网络A的性能。所以，我们不再B-E的网络上使用LRN。\n",
    "\n",
    "Second, we observe that the classification error decreases with the increased ConvNet depth: from 11 layers in A to 19 layers in E. Notably, in spite of the same depth, the configuration C (which contains three 1 × 1 conv. layers), performs worse than the configuration D, which uses 3 × 3 conv. layers throughout the network. This indicates that while the additional non-linearity does help (C is better than B), it is also important to capture spatial context by using conv. filters with non-trivial receptive fields (D is better than C). The error rate of our architecture saturates when the depth reaches 19 layers, but even deeper models might be beneficial for larger datasets. We also compared the net B with a shallow net with five 5 × 5 conv. layers, which was derived from B by replacing each pair of 3 × 3 conv. layers with a single 5 × 5 conv. layer (which has the same receptive field as explained in Sect. 2.3). The top-1 error of the shallow net was measured to be 7% higher than that of B (on a center crop), which confirms that a deep net with small filters outperforms a shallow net with larger filters.\n",
    "\n",
    "第二，我们观察到随着深度的增加分类误差在减少：从A11层到E19层。值得一提的是，C和D的深度一致，但包含了三个1x1conv的C比使用3x3conv的D性能要差。这表明额外的非线性是有帮助的，但是使用能捕获空间上下文的感知域也很重要（D比C要好）。错误率在深度达到19层之后停滞了，但是对于更大的数据集更深的网络可能是有益的。我们还对比了网路B和一个有5x5conv的浅网络，这个网络把每两个3x3conv替换成一个5x5conv（2.3节解释了这两种方法具有相同的感知域）。top-1错误率在浅网络上比B上要高出7%，这就证实了一个使用小卷积核的深网络要比一个使用大卷积核的浅网络要好。\n",
    "\n",
    "Finally, scale jittering at training time (S ∈ [256; 512]) leads to significantly better results than training on images with fixed smallest side (S = 256 or S = 384), even though a single scale is used at test time. This confirms that training set augmentation by scale jittering is indeed helpful for capturing multi-scale image statistics.\n",
    "\n",
    "最后，使用多尺度训练得到的效果明显的好于单尺度的，即使在测试时只用一个尺度。这就说明了使用多尺度变化来做训练数据集的增量是确实有效的，能捕获多尺度图像的特征。\n",
    "\n",
    "Table 3: ConvNet performance at a single test scale.\n",
    "\n",
    "![vgg-performance](vgg-perf.png)\n",
    "\n",
    "4.2 MULTI-SCALE EVALUATION\n",
    "\n",
    "Having evaluated the ConvNet models at a single scale, we now assess the effect of scale jittering at test time. It consists of running a model over several rescaled versions of a test image (corresponding to different values of Q), followed by averaging the resulting class posteriors. Considering that a large discrepancy between training and testing scales leads to a drop in performance, the models trained with fixed S were evaluated over three test image sizes, close to the training one: Q = {S − 32, S, S + 32}. At the same time, scale jittering at training time allows the network to be applied to a wider range of scales at test time, so the model trained with variable S ∈ [Smin; Smax] was evaluated over a larger range of sizes Q = {Smin, 0.5(Smin + Smax), Smax}.\n",
    "\n",
    "已经计算了单尺度时的模型。现在我们来评估多尺度测试。这个过程包括，在测试图片上进行多个拉伸（对应多个Q值）来运行模型。然后把着多个结果取平均得到最后的分类结果。在训练和测试上的巨大区别会导致性能的下降，例如使用固定的S来训练，而使用三个Q：{S-32，S，S+32}。同时，多尺度训练使网络在测试阶段能应用到更广的尺度范围。在训练的时候使用S ∈ [Smin; Smax]，测试的时候使用Q = {Smin，0.5(Smin + Smax)，Smax}。\n",
    "\n",
    "The results, presented in Table 4, indicate that scale jittering at test time leads to better performance (as compared to evaluating the same model at a single scale, shown in Table 3). As before, the deepest configurations (D and E) perform the best, and scale jittering is better than training with a fixed smallest side S. Our best single-network performance on the validation set is 24.8%/7.5% top-1/top-5 error (highlighted in bold in Table 4). On the test set, the configuration E achieves 7.3% top-5 error.\n",
    "\n",
    "结果显示在表4中。结果表明使用多尺度测试的时候效果更好。像之前一样，最深的网络D和E表现最好，多尺度比单尺度摇号。我们最好的单网络在验证集上最好的结果为top-1/top-5错误率分别为 24.8%/7.5%（表格4中粗体）。在测试集上，E达到了7.3%的top-5错误率。\n",
    "\n",
    "Table 4: ConvNet performance at multiple test scales.\n",
    "\n",
    "![vgg-perf-multi-scale](vgg-perf-multi-scale.png)\n",
    "\n",
    "4.3 MULTI-CROP EVALUATION\n",
    "\n",
    "In Table 5 we compare dense ConvNet evaluation with mult-crop evaluation (see Sect. 3.2 for details). We also assess the complementarity of the two evaluation techniques by averaging their softmax outputs. As can be seen, using multiple crops performs slightly better than dense evaluation, and the two approaches are indeed complementary, as their combination outperforms each of them. As noted above, we hypothesize that this is due to a different treatment of convolution boundary conditions.\n",
    "\n",
    "在表5中，我们比较了dense convnet求值和多截取求值（详见3.2节）。我们还通过平均它们的softmax输出来评估它们的互补性。可以看出，使用多截取比dense eval要稍稍好一些，并且这两种方法是互补的，结合使用效果更好。我们假设这是因为不同的对待卷积边界条件的方式。\n",
    "\n",
    "Table 5: ConvNet evaluation techniques comparison. In all experiments the training scale S was sampled from [256; 512], and three test scales Q were considered: {256, 384, 512}.\n",
    "\n",
    "![vgg-eval](vgg-eval.png)\n",
    "\n",
    "4.4 CONVNET FUSION\n",
    "\n",
    "Up until now, we evaluated the performance of individual ConvNet models. In this part of the experiments, we combine the outputs of several models by averaging their soft-max class posteriors. This improves the performance due to complementarity of the models, and was used in the top ILSVRC submissions in 2012 (Krizhevsky et al., 2012) and 2013 (Zeiler & Fergus, 2013; Sermanet et al., 2014).\n",
    "\n",
    "这部分，我们把不同模型的输出平均来得到分类。这样可以改进性能，因为模型的互补。这也被使用在Krizhevsky2012和Zeriler&Fergus2013和Sermanet2014中。\n",
    "\n",
    "The results are shown in Table 6. By the time of ILSVRC submission we had only trained the single-scale networks, as well as a multi-scale model D (by fine-tuning only the fully-connected layers rather than all layers). The resulting ensemble of 7 networks has 7.3% ILSVRC test error. After the submission, we considered an ensemble of only two best-performing multi-scale models (configurations D and E), which reduced the test error to 7.0% using dense evaluation and 6.8% using combined dense and multi-crop evaluation. For reference, our best-performing single model achieves 7.1% error (model E, Table 5).\n",
    "\n",
    "结果在表6中。\n",
    "\n",
    "4.5 COMPARISON WITH THE STATE OF THE ART\n",
    "\n",
    "Finally, we compare our results with the state of the art in Table 7. In the classification task of ILSVRC-2014 challenge (Russakovsky et al., 2014), our “VGG” team secured the 2nd place with 7.3% test error using an ensemble of 7 models. After the submission, we decreased the error rate to 6.8% using an ensemble of 2 models.\n",
    "\n",
    "Table 6: Multiple ConvNet fusion results.\n",
    "\n",
    "As can be seen from Table 7, our very deep ConvNets significantly outperform the previous generation of models, which achieved the best results in the ILSVRC-2012 and ILSVRC-2013 competi- tions. Our result is also competitive with respect to the classification task winner (GoogLeNet with 6.7% error) and substantially outperforms the ILSVRC-2013 winning submission Clarifai, which achieved 11.2% with outside training data and 11.7% without it. This is remarkable, considering that our best result is achieved by combining just two models – significantly less than used in most ILSVRC submissions. In terms of the single-net performance, our architecture achieves the best result (7.0% test error), outperforming a single GoogLeNet by 0.9%. Notably, we did not depart from the classical ConvNet architecture of LeCun et al. (1989), but improved it by substantially increasing the depth.\n",
    "\n",
    "Table 7: Comparison with the state of the art in ILSVRC classification. Our method is denoted as “VGG”. Only the results obtained without outside training data are reported.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 CONCLUSION\n",
    "\n",
    "In this work we evaluated very deep convolutional networks (up to 19 weight layers) for large-scale image classification. It was demonstrated that the representation depth is beneficial for the classification accuracy, and that state-of-the-art performance on the ImageNet challenge dataset can be achieved using a conventional ConvNet architecture (LeCun et al., 1989; Krizhevsky et al., 2012) with substantially increased depth. In the appendix, we also show that our models generalise well to a wide range of tasks and datasets, matching or outperforming more complex recognition pipelines built around less deep image representations. Our results yet again confirm the importance of depth in visual representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
