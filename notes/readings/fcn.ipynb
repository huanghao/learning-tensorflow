{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fully Convolutional Networks for Semantic Segmentation\n",
    "----\n",
    "\n",
    "arXiv:1411.4038v2 [cs.CV] 8 Mar 2015\n",
    "\n",
    "https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abstract\n",
    "\n",
    "Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build “fully convolutional” networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet [19], the VGG net [31], and GoogLeNet [32]) into fully convolutional networks and transfer their learned representations by fine-tuning [4] to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20% relative improvement to 62.2% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.\n",
    "\n",
    "convnet是生层结构化的特征的强大视觉模型。convnet通过端到端，像素到像素的训练，已经超越了语义分割的最高水平。我们的关键贡献在于构建一个“全卷积”网络，接收任意尺寸的输入，在有效的推理和学习下，产生对应大小的输出。我们定义和详细描述全卷积网络，把它应用到空间稠密的预测任务上，和之前模型的联系。我们采用当前的分类网络（AlexNet，VGGnet，GooLeNet）转变为fcn，然后通过迁移学习和调优来用作分割任务。从而我们定义了一个新的架构，它结合了来自深度粗糙层的语义信息，和浅的细致的表面信息，来产生精确的细节的分割。我们的fcn在PASCAL VOC的分割任务上达到了先进的水平（20%的相对改进，相比于62.2% mean UI 2012年）NYUDv2和SIFT Flow，并且对于一张典型的图片推理需要不到0.2秒的时间。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Introduction\n",
    "\n",
    "Convolutional networks are driving advances in recognition. Convnets are not only improving for whole-image classification [19, 31, 32], but also making progress on local tasks with structured output. These include advances in bounding box object detection [29, 12, 17], part and keypoint prediction [39, 24], and local correspondence [24, 9].\n",
    "\n",
    "convnet在识别上持续改进。convnet不仅仅改进了整张图片的分类[19,31,32]，并且通过结构化的输出在定位任务上做出了改进。这包括边界框物体检测[29,12,17]，part and keypoint预测[39,24]，和local correspondence[24,9]。\n",
    "\n",
    "The natural next step in the progression from coarse to fine inference is to make a prediction at every pixel. Prior approaches have used convnets for semantic segmentation [27, 2, 8, 28, 16, 14, 11], in which each pixel is labeled with the class of its enclosing object or region, but with shortcomings that this work addresses.\n",
    "\n",
    "自然地，下一步从粗糙到细致的改进就是在每个像素点进行预测。之前的工作使用convnet做语义分割[27,2,8,28,16,14,11]，每个像素点被标记为包含它的物体或者区域的类型，但是有一些本文将解决的缺点。\n",
    "\n",
    "We show that a fully convolutional network (FCN), trained end-to-end, pixels-to-pixels on semantic segmentation exceeds the state-of-the-art without further machinery. To our knowledge, this is the first work to train FCNs end-to-end (1) for pixelwise prediction and (2) from supervised pre-training. Fully convolutional versions of existing networks predict dense outputs from arbitrary-sized inputs. Both learning and inference are performed whole-image-at-a-time by dense feedforward computation and backpropagation. In-network upsampling layers enable pixelwise prediction and learning in nets with subsampled pooling.\n",
    "\n",
    "我们展示了一个fcn，端到端，像素到像素进行训练，在语义分割上达到最先进的水平，而不需要更多的机器。在我们的知识里，这是第一次训练fcn (1)在像素级别做预测 (2) 进行监督预训练。目前的全卷积网络从任何尺度的输入预测出稠密的输出。训练和推理都以“一整张图片一次”进行稠密的前向和反向计算来执行。网路中，上采样层使得在池化降采样后的网路进行像素级的预测和学习成为可能。\n",
    "\n",
    "This method is efficient, both asymptotically and absolutely, and precludes the need for the complications in other works. Patchwise training is common [27, 2, 8, 28, 11], but lacks the efficiency of fully convolutional training. Our approach does not make use of preand post-processing complications, including superpixels [8, 16], proposals [16, 14], or post-hoc refinement by random fields or local classifiers [8, 16]. Our model transfers recent success in classification [19, 31, 32] to dense prediction by reinterpreting classification nets as fully convolutional and fine-tuning from their learned representations. In contrast, previous works have applied small convnets without supervised pre-training [8, 28, 27].\n",
    "\n",
    "Semantic segmentation faces an inherent tension between semantics and location: global information resolves what while local information resolves where. Deep feature hierarchies jointly encode location and semantics in a localto-global pyramid. We define a novel “skip” architecture to combine deep, coarse, semantic information and shallow, fine, appearance information in Section 4.2 (see Figure 3).\n",
    "\n",
    "In the next section, we review related work on deep classification nets, FCNs, and recent approaches to semantic segmentation using convnets. The following sections explain FCN design and dense prediction tradeoffs, introduce our architecture with in-network upsampling and multilayer combinations, and describe our experimental framework. Finally, we demonstrate state-of-the-art results on PASCAL VOC 2011-2, NYUDv2, and SIFT Flow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 Related work\n",
    "\n",
    "Our approach draws on recent successes of deep nets for image classification [19, 31, 32] and transfer learning [4, 38]. Transfer was first demonstrated on various visual recognition tasks [4, 38], then on detection, and on both instance and semantic segmentation in hybrid proposalclassifier models [12, 16, 14]. We now re-architect and finetune classification nets to direct, dense prediction of semantic segmentation. We chart the space of FCNs and situate prior models, both historical and recent, in this framework.\n",
    "\n",
    "Fully convolutional networks To our knowledge, the idea of extending a convnet to arbitrary-sized inputs first appeared in Matan et al. [25], which extended the classic LeNet [21] to recognize strings of digits. Because their net was limited to one-dimensional input strings, Matan et al. used Viterbi decoding to obtain their outputs. Wolf and Platt [37] expand convnet outputs to 2-dimensional maps of detection scores for the four corners of postal address blocks. Both of these historical works do inference and learning fully convolutionally for detection. Ning et al. [27] define a convnet for coarse multiclass segmentation of C. elegans tissues with fully convolutional inference.\n",
    "\n",
    "Fully convolutional computation has also been exploited in the present era of many-layered nets. Sliding window detection by Sermanet et al. [29], semantic segmentation by Pinheiro and Collobert [28], and image restoration by Eigen et al. [5] do fully convolutional inference. Fully convolutional training is rare, but used effectively by Tompson et al. [35] to learn an end-to-end part detector and spatial model for pose estimation, although they do not exposit on or analyze this method.\n",
    "\n",
    "Alternatively, He et al. [17] discard the nonconvolutional portion of classification nets to make a feature extractor. They combine proposals and spatial pyramid pooling to yield a localized, fixed-length feature for classification. While fast and effective, this hybrid model cannot be learned end-to-end.\n",
    "\n",
    "Dense prediction with convnets Several recent works have applied convnets to dense prediction problems, including semantic segmentation by Ning et al. [27], Farabet et al.[8], and Pinheiro and Collobert [28]; boundary prediction for electron microscopy by Ciresan et al. [2] and for natural images by a hybrid neural net/nearest neighbor model by Ganin and Lempitsky [11]; and image restoration and depth estimation by Eigen et al. [5, 6]. Common elements of these approaches include\n",
    "\n",
    "• small models restricting capacity and receptive fields;\n",
    "\n",
    "• patchwise training [27, 2, 8, 28, 11];\n",
    "\n",
    "• post-processing by superpixel projection, random field\n",
    "\n",
    "regularization, filtering, or local classification [8, 2,\n",
    "\n",
    "11];\n",
    "\n",
    "• input shifting and output interlacing for dense output\n",
    "\n",
    "[28, 11] as introduced by OverFeat [29];\n",
    "\n",
    "• multi-scale pyramid processing [8, 28, 11];\n",
    "\n",
    "• saturating tanh nonlinearities [8, 5, 28]; and\n",
    "\n",
    "• ensembles [2, 11],\n",
    "\n",
    "whereas our method does without this machinery. However, we do study patchwise training 3.4 and “shift-and-stitch” dense output 3.2 from the perspective of FCNs. We also discuss in-network upsampling 3.3, of which the fully connected prediction by Eigen et al. [6] is a special case.\n",
    "\n",
    "Unlike these existing methods, we adapt and extend deep classification architectures, using image classification as supervised pre-training, and fine-tune fully convolutionally to learn simply and efficiently from whole image inputs and whole image ground thruths.\n",
    "\n",
    "Hariharan et al. [16] and Gupta et al. [14] likewise adapt deep classification nets to semantic segmentation, but do so in hybrid proposal-classifier models. These approaches fine-tune an R-CNN system [12] by sampling bounding boxes and/or region proposals for detection, semantic segmentation, and instance segmentation. Neither method is learned end-to-end.\n",
    "\n",
    "They achieve state-of-the-art results on PASCAL VOC segmentation and NYUDv2 segmentation respectively, so we directly compare our standalone, end-to-end FCN to their semantic segmentation results in Section 5.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 Fully convolutional networks\n",
    "\n",
    "Each layer of data in a convnet is a three-dimensional array of size h × w × d, where h and w are spatial dimensions, and d is the feature or channel dimension. The first layer is the image, with pixel size h × w, and d color channels. Locations in higher layers correspond to the locations in the image they are path-connected to, which are called their receptive fields.\n",
    "\n",
    "convnet中每一层的数据是一个h x w x d的三维数组，h和w是空间维度，d是特征/通道维度。第一层是像素大小为h x w的图片，d个颜色通道。高层中的位置和底层中的位置是路径相连的，这叫做它们的感知域。\n",
    "\n",
    "Convnets are built on translation invariance. Their basic components (convolution, pooling, and activation functions) operate on local input regions, and depend only on relative spatial coordinates. Writing xij for the data vector at location (i, j) in a particular layer, and yij for the following layer, these functions compute outputs yij by yij = fks ({xsi+δi,sj+δj}0≤δi,δj≤k) where k is called the kernel size, s is the stride or subsampling factor, and fks determines the layer type: a matrix multiplication for convolution or average pooling, a spatial max for max pooling, or an elementwise nonlinearity for an activation function, and so on for other types of layers.\n",
    "\n",
    "convnet是基于变换无关的。基本的组件（conv, pooling和激励函数）操作局部的输入区域，而且只依赖于相对的空间坐标。$x_{ij}$是某层中(i,j)位置的数据向量，$y_{ij}$是下一层的，计算公式为$$ y_{ij} = f_{ks} (\\{ x_{s i + \\delta i, s j + \\delta j} \\} 0 \\le \\delta i, \\delta j \\le k) $$ 其中k是卷积核的大小，s是步长，或者下采样因子，$f_{ks}$决定了层的类型：conv或者avpool是一个矩阵乘法，maxpool是一个空间的max，对于激励函数是一个元素层次的非线性，其他的层次也类似等等。\n",
    "\n",
    "This functional form is maintained under composition, with kernel size and stride obeying the transformation rule fks ◦ gk′s′ = (f ◦ g)k′+(k−1)s′,ss′ .\n",
    "\n",
    "这种函数形式通过组合实现，卷积核大小和步长遵守变换规则$$ f_{ks} \\circ g_{k's'} = (f \\circ g)_{k' + (k-1)s', ss'.} $$.\n",
    "\n",
    "While a general deep net computes a general nonlinear function, a net with only layers of this form computes a nonlinear filter, which we call a deep filter or fully convolutional network. An FCN naturally operates on an input of any size, and produces an output of corresponding (possibly resampled) spatial dimensions.\n",
    "\n",
    "通用的深度网络当前于通用的非线性函数，一个只包含这种形式层的网络计算一个非线性滤波器，我们成为深度滤波器或者全卷积网络。一个FCN可以接收任意尺寸的输入，产生对应空间维度（有可能变化）的输出。\n",
    "\n",
    "A real-valued loss function composed with an FCN defines a task. If the loss function is a sum over the spatial dimensions of the final layer, l(x; θ) =  ij l′(xij ; θ), its gradient will be a sum over the gradients of each of its spatial components. Thus stochastic gradient descent on l computed on whole images will be the same as stochastic gradient descent on l′, taking all of the final layer receptive fields as a minibatch.\n",
    "\n",
    "一个FCN加一个实数损失函数就定义了一个任务。如果这个损失函数是最后一层空间维度上的加和，$\\mathscr{l}(x; \\theta) = \\sum_{ij} \\mathscr{l'}(x_{ij}; \\theta)$，它的梯度将是每个空间组件的梯度的和。这样SGD对整张图片在$\\mathscr{l}$的上计算就会和在$\\mathscr{l'}$上SGD一样，把最后一层的感知域当成一个minibatch。\n",
    "\n",
    "When these receptive fields overlap significantly, both feedforward computation and backpropagation are much more efficient when computed layer-by-layer over an entire image instead of independently patch-by-patch.\n",
    "\n",
    "We next explain how to convert classification nets into fully convolutional nets that produce coarse output maps. For pixelwise prediction, we need to connect these coarse outputs back to the pixels. Section 3.2 describes a trick that OverFeat [29] introduced for this purpose. We gain insight into this trick by reinterpreting it as an equivalent network modification. As an efficient, effective alternative, we introduce deconvolution layers for upsampling in Section 3.3. In Section 3.4 we consider training by patchwise sampling, and give evidence in Section 4.3 that our whole image training is faster and equally effective.\n",
    "\n",
    "3.1. Adapting classifiers for dense prediction\n",
    "\n",
    "Typical recognition nets, including LeNet [21], AlexNet [19], and its deeper successors [31, 32], ostensibly take fixed-sized inputs and produce nonspatial outputs. The fully connected layers of these nets have fixed dimensions and throw away spatial coordinates. However, these fully connected layers can also be viewed as convolutions with kernels that cover their entire input regions. Doing so casts them into fully convolutional networks that take input of any size and output classification maps. This transformation is illustrated in Figure 2. (By contrast, nonconvolutional nets, such as the one by Le et al. [20], lack this capability.)\n",
    "\n",
    "典型的识别网络，包括LeNet，AlexNet和它的更深的后继者，接受固定尺寸的输入，产生非空间的输出。fc层有固定的维度，扔掉了空间的坐标。但是，这些fc层也可以被看做conv层，它的卷积核覆盖了整个输入的区域。这样就把它们转变成了FCN接收任意尺寸的输入，输出分类图。这个转换显示在图2中。（相反的，非卷积网络，例如[20]就没有这种能力）\n",
    "\n",
    "Figure 2. Transforming fully connected layers into convolution layers enables a classification net to output a heatmap. Adding layers and a spatial loss (as in Figure 1) produces an efficient machine for end-to-end dense learning.\n",
    "\n",
    "图2：把fc变换成conv，使分类网络输出一个热力图。增加更多的层和空间损失函数，产生一个有效的机器用来做端到端的稠密学习。\n",
    "\n",
    "![fcn_fig2_fc2conv.png](fcn_fig2_fc2conv.png)\n",
    "\n",
    "Furthermore, while the resulting maps are equivalent to the evaluation of the original net on particular input patches, the computation is highly amortized over the overlapping regions of those patches. For example, while AlexNet takes 1.2 ms (on a typical GPU) to produce the classification scores of a 227 × 227 image, the fully convolutional version takes 22 ms to produce a 10 × 10 grid of outputs from a 500 × 500 image, which is more than 5 times faster than the na ̈ıve approach1.\n",
    "\n",
    "此外，得到的结果和原始网络在特定输入区域的评估是一致的，但是计算被高度分摊在这些重叠的区域。例如，AlexNet需要1.2ms（在一个典型的gpu上）产生一个227x227的分类得分，而全卷积版本需要22ms来对一个500x500的输入图片产生一个10x10的输出网格，这比原始的方法快了5倍还多。\n",
    "\n",
    "1Assuming efficient batching of single image inputs. The classification scores for a single image by itself take 5.4 ms to produce, which is nearly 25 times slower than the fully convolutional version.\n",
    "\n",
    "1假设有效的单图片的输入批操作。对一个单图片的分类需要5.4ms，比全卷积版本慢了差不多25倍。\n",
    "\n",
    "The spatial output maps of these convolutionalized models make them a natural choice for dense problems like semantic segmentation. With ground truth available at every output cell, both the forward and backward passes are straightforward, and both take advantage of the inherent computational efficiency (and aggressive optimization) of convolution.\n",
    "\n",
    "这些卷积模型的空间输出图使得它们成为像语义分割这样的稠密问题是个天然的选择。在每个输出的网格内的有真值，前向和后向计算都很直接，它们都能利用卷积的股友计算效率（和激进的优化方法）。\n",
    "\n",
    "The corresponding backward times for the AlexNet example are 2.4 ms for a single image and 37 ms for a fully convolutional 10 × 10 output map, resulting in a speedup similar to that of the forward pass. This dense backpropagation is illustrated in Figure 1.\n",
    "\n",
    "AlexNet对于单个图片的反向计算是2.4ms，对于fcn 10x10的输出图需要37ms，和正向的提速是类似的。这种稠密的反向传递在图1中显示。\n",
    "\n",
    "Figure 1. Fully convolutional networks can efficiently learn to make dense predictions for per-pixel tasks like semantic segmentation.\n",
    "\n",
    "图1：FCN可以高效地学习做稠密预测，像素级的任务例如语义分割。\n",
    "\n",
    "![fcn_fig1_semantic_segmentation.png](fcn_fig1_semantic_segmentation.png)\n",
    "\n",
    "While our reinterpretation of classification nets as fully convolutional yields output maps for inputs of any size, the output dimensions are typically reduced by subsampling. The classification nets subsample to keep filters small and computational requirements reasonable. This coarsens the output of a fully convolutional version of these nets, reducing it from the size of the input by a factor equal to the pixel stride of the receptive fields of the output units.\n",
    "\n",
    "虽然我们把分类网络重新解释成全卷积网络，使得输出可以由任何尺寸的输入产生，而输出维度一般都通过降采样来减少。分类网络降采样保持小卷积核和可接受的计算量。这粗糙化了这些网络的全卷积版本的输出，把输入减小的比率等于输出单元的感知域的像素步长的大小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "3.2 Shift-and-stitch is filter rarefaction\n",
    "\n",
    "Input shifting and output interlacing is a trick that yields dense predictions from coarse outputs without interpolation, introduced by OverFeat [29]. If the outputs are downsampled by a factor of f , the input is shifted (by left and top padding) x pixels to the right and y pixels down, once for every value of (x,y) ∈ {0,...,f − 1} × {0,...,f − 1}. These f2 inputs are each run through the convnet, and the outputs are interlaced so that the predictions correspond to the pixels at the centers of their receptive fields.\n",
    "\n",
    "Changing only the filters and layer strides of a convnet can produce the same output as this shift-and-stitch trick. Consider a layer (convolution or pooling) with input stride s, and a following convolution layer with filter weights fij (eliding the feature dimensions, irrelevant here). Setting the lower layer’s input stride to 1 upsamples its output by a factor of s, just like shift-and-stitch. However, convolving the original filter with the upsampled output does not produce the same result as the trick, because the original filter only sees a reduced portion of its (now upsampled) input. To reproduce the trick, rarefy the filter by enlarging it as\n",
    "\n",
    "′   fi/s,j/s ifsdividesbothiandj; fij = 0 otherwise,\n",
    "\n",
    "(with i and j zero-based). Reproducing the full net output of the trick involves repeating this filter enlargement layerby-layer until all subsampling is removed.\n",
    "\n",
    "Simply decreasing subsampling within a net is a tradeoff: the filters see finer information, but have smaller receptive fields and take longer to compute. We have seen that the shift-and-stitch trick is another kind of tradeoff: the output is made denser without decreasing the receptive field sizes of the filters, but the filters are prohibited from accessing information at a finer scale than their original design.\n",
    "\n",
    "Although we have done preliminary experiments with shift-and-stitch, we do not use it in our model. We find learning through upsampling, as described in the next section, to be more effective and efficient, especially when combined with the skip layer fusion described later on.\n",
    "\n",
    "3.3. Upsampling is backwards strided convolution\n",
    "\n",
    "Another way to connect coarse outputs to dense pixels is interpolation. For instance, simple bilinear interpolation computes each output yij from the nearest four inputs by a linear map that depends only on the relative positions of the input and output cells.\n",
    "\n",
    "In a sense, upsampling with factor f is convolution with a fractional input stride of 1/f. So long as f is integral, a natural way to upsample is therefore backwards convolution (sometimes called deconvolution) with an output stride of f . Such an operation is trivial to implement, since it simply reverses the forward and backward passes of convolution.\n",
    "\n",
    "Thus upsampling is performed in-network for end-to-end learning by backpropagation from the pixelwise loss.\n",
    "\n",
    "Note that the deconvolution filter in such a layer need not be fixed (e.g., to bilinear upsampling), but can be learned. A stack of deconvolution layers and activation functions can even learn a nonlinear upsampling.\n",
    "\n",
    "In our experiments, we find that in-network upsampling is fast and effective for learning dense prediction. Our best segmentation architecture uses these layers to learn to upsample for refined prediction in Section 4.2.\n",
    "\n",
    "3.4. Patchwise training is loss sampling\n",
    "\n",
    "In stochastic optimization, gradient computation is driven by the training distribution. Both patchwise training and fully-convolutional training can be made to produce any distribution, although their relative computational efficiency depends on overlap and minibatch size. Whole image fully convolutional training is identical to patchwise training where each batch consists of all the receptive fields of the units below the loss for an image (or collection of images). While this is more efficient than uniform sampling of patches, it reduces the number of possible batches. However, random selection of patches within an image may be recovered simply. Restricting the loss to a randomly sampled subset of its spatial terms (or, equivalently applying a DropConnect mask [36] between the output and the loss) excludes patches from the gradient computation.\n",
    "\n",
    "If the kept patches still have significant overlap, fully convolutional computation will still speed up training. If gradients are accumulated over multiple backward passes, batches can include patches from several images.2\n",
    "\n",
    "Sampling in patchwise training can correct class imbalance [27, 8, 2] and mitigate the spatial correlation of dense patches [28, 16]. In fully convolutional training, class balance can also be achieved by weighting the loss, and loss sampling can be used to address spatial correlation.\n",
    "\n",
    "We explore training with sampling in Section 4.3, and do not find that it yields faster or better convergence for dense prediction. Whole image training is effective and efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 Segmentation Architecture\n",
    "\n",
    "We cast ILSVRC classifiers into FCNs and augment them for dense prediction with in-network upsampling and a pixelwise loss. We train for segmentation by fine-tuning. Next, we build a novel skip architecture that combines coarse, semantic and local, appearance information to refine prediction.\n",
    "\n",
    "For this investigation, we train and validate on the PASCAL VOC 2011 segmentation challenge [7]. We train with a per-pixel multinomial logistic loss and validate with the standard metric of mean pixel intersection over union, with the mean taken over all classes, including background. The training ignores pixels that are masked out (as ambiguous or difficult) in the ground truth.\n",
    "\n",
    "4.1. From classifier to dense FCN\n",
    "\n",
    "We begin by convolutionalizing proven classification architectures as in Section 3. We consider the AlexNet3 architecture [19] that won ILSVRC12, as well as the VGG nets [31] and the GoogLeNet4 [32] which did exceptionally well in ILSVRC14. We pick the VGG 16-layer net5, which we found to be equivalent to the 19-layer net on this task. For GoogLeNet, we use only the final loss layer, and improve performance by discarding the final average pooling layer. We decapitate each net by discarding the final classifier layer, and convert all fully connected layers to convolutions. We append a 1 × 1 convolution with channel dimension 21 to predict scores for each of the PASCAL classes (including background) at each of the coarse output locations, followed by a deconvolution layer to bilinearly upsample the coarse outputs to pixel-dense outputs as described in Section 3.3. Table 1 compares the preliminary validation results along with the basic characteristics of each net. We report the best results achieved after convergence at a fixed learning rate (at least 175 epochs).\n",
    "\n",
    "Fine-tuning from classification to segmentation gave reasonable predictions for each net. Even the worst model achieved ∼ 75% of state-of-the-art performance. The segmentation-equippped VGG net (FCN-VGG16) already appears to be state-of-the-art at 56.0 mean IU on val, compared to 52.6 on test [16]. Training on extra data raises performance to 59.4 mean IU on a subset of val7. Training details are given in Section 4.3.\n",
    "\n",
    "Despite similar classification accuracy, our implementation of GoogLeNet did not match this segmentation result.\n",
    "\n",
    "4.2. Combining what and where\n",
    "\n",
    "We define a new fully convolutional net (FCN) for segmentation that combines layers of the feature hierarchy and refines the spatial precision of the output. See Figure 3.\n",
    "\n",
    "While fully convolutionalized classifiers can be finetuned to segmentation as shown in 4.1, and even score highly on the standard metric, their output is dissatisfyingly coarse (see Figure 4). The 32 pixel stride at the final prediction layer limits the scale of detail in the upsampled output.\n",
    "\n",
    "We address this by adding links that combine the final prediction layer with lower layers with finer strides. This turns a line topology into a DAG, with edges that skip ahead from lower layers to higher ones (Figure 3). As they see fewer pixels, the finer scale predictions should need fewer layers, so it makes sense to make them from shallower net outputs. Combining fine layers and coarse layers lets the model make local predictions that respect global structure. By analogy to the multiscale local jet of Florack et al. [10], we call our nonlinear local feature hierarchy the deep jet.\n",
    "\n",
    "We first divide the output stride in half by predicting from a 16 pixel stride layer. We add a 1 × 1 convolution layer on top of pool4 to produce additional class predictions. We fuse this output with the predictions computed on top of conv7 (convolutionalized fc7) at stride 32 by adding a 2× upsampling layer and summing6 both predictions. (See Figure 3). We initialize the 2× upsampling to bilinear interpolation, but allow the parameters to be learned as described in Section 3.3. Finally, the stride 16 predictions are upsampled back to the image. We call this net FCN-16s. FCN-16s is learned end-to-end, initialized with the parameters of the last, coarser net, which we now call FCN-32s. The new parameters acting on pool4 are zero-initialized so that the net starts with unmodified predictions. The learning rate is decreased by a factor of 100.\n",
    "\n",
    "Learning this skip net improves performance on the validation set by 3.0 mean IU to 62.4. Figure 4 shows improvement in the fine structure of the output. We compared this fusion with learning only from the pool4 layer (which resulted in poor performance), and simply decreasing the learning rate without adding the extra link (which results in an insignificant performance improvement, without improving the quality of the output).\n",
    "\n",
    "We continue in this fashion by fusing predictions from pool3 with a 2× upsampling of predictions fused from pool4 and conv7, building the net FCN-8s. We obtain a minor additional improvement to 62.7 mean IU, and find a slight improvement in the smoothness and detail of our output. At this point our fusion improvements have met diminishing returns, both with respect to the IU metric which emphasizes large-scale correctness, and also in terms of the improvement visible e.g. in Figure 4, so we do not continue fusing even lower layers.\n",
    "\n",
    "Refinement by other means Decreasing the stride of pooling layers is the most straightforward way to obtain finer predictions. However, doing so is problematic for our VGG16-based net. Setting the pool5 layer to have stride 1 requires our convolutionalized fc6 to have a kernel size of 14 × 14 in order to maintain its receptive field size. In addition to their computational cost, we had difficulty learning such large filters. We made an attempt to re-architect the layers above pool5 with smaller filters, but were not successful in achieving comparable performance; one possible explanation is that the initialization from ImageNet-trained weights in the upper layers is important.\n",
    "\n",
    "Another way to obtain finer predictions is to use the shiftand-stitch trick described in Section 3.2. In limited experiments, we found the cost to improvement ratio from this method to be worse than layer fusion.\n",
    "\n",
    "4.3. Experimental framework\n",
    "\n",
    "Optimization We train by SGD with momentum. We use a minibatch size of 20 images and fixed learning rates of 10−3, 10−4, and 5−5 for FCN-AlexNet, FCN-VGG16, and FCN-GoogLeNet, respectively, chosen by line search. We use momentum 0.9, weight decay of 5−4 or 2−4, and doubled the learning rate for biases, although we found training to be insensitive to these parameters (but sensitive to the learning rate). We zero-initialize the class scoring convolution layer, finding random initialization to yield neither better performance nor faster convergence. Dropout was included where used in the original classifier nets.\n",
    "\n",
    "Fine-tuning We fine-tune all layers by backpropagation through the whole net. Fine-tuning the output classifier alone yields only 70% of the full finetuning performance as compared in Table 2. Training from scratch is not feasible considering the time required to learn the base classification nets. (Note that the VGG net is trained in stages, while we initialize from the full 16-layer version.) Fine-tuning takes three days on a single GPU for the coarse FCN-32s version, and about one day each to upgrade to the FCN-16s and FCN-8s versions.\n",
    "\n",
    "Patch Sampling As explained in Section 3.4, our full image training effectively batches each image into a regular grid of large, overlapping patches. By contrast, prior work randomly samples patches over a full dataset [27, 2, 8, 28, 11], potentially resulting in higher variance batches that may accelerate convergence [22]. We study this tradeoff by spatially sampling the loss in the manner described earlier, making an independent choice to ignore each final layer cell with some probability 1−p. To avoid changing the effective batch size, we simultaneously increase the number of images per batch by a factor 1/p. Note that due to the efficiency of convolution, this form of rejection sampling is still faster than patchwise training for large enough values of p (e.g., at least for p > 0.2 according to the numbers in Section 3.1). Figure 5 shows the effect of this form of sampling on convergence. We find that sampling does not have a significant effect on convergence rate compared to whole image training, but takes significantly more time due to the larger number of images that need to be considered per batch. We therefore choose unsampled, whole image training in our other experiments.\n",
    "\n",
    "Class Balancing Fully convolutional training can balance classes by weighting or sampling the loss. Although our labels are mildly unbalanced (about 3/4 are background), we find class balancing unnecessary.\n",
    "\n",
    "Dense Prediction The scores are upsampled to the input dimensions by deconvolution layers within the net. Final layer deconvolutional filters are fixed to bilinear interpolation, while intermediate upsampling layers are initialized to bilinear upsampling, and then learned. Shift-andstitch (Section 3.2), or the filter rarefaction equivalent, are not used.\n",
    "\n",
    "Augmentation We tried augmenting the training data by randomly mirroring and “jittering” the images by translating them up to 32 pixels (the coarsest scale of prediction) in each direction. This yielded no noticeable improvement.\n",
    "\n",
    "MoreTrainingData ThePASCALVOC2011segmentation challenge training set, which we used for Table 1, labels 1112 images. Hariharan et al. [15] have collected labels for a much larger set of 8498 PASCAL training images, which was used to train the previous state-of-the-art system, SDS [16]. This training data improves the FCNVGG16 validation score7 by 3.4 points to 59.4 mean IU.\n",
    "\n",
    "Implementation Allmodelsaretrainedandtestedwith Caffe [18] on a single NVIDIA Tesla K40c. The models and code will be released open-source on publication."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
