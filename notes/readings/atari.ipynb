{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Playing Atari with Deep Reinforcement Learning\n",
    "====\n",
    "\n",
    "19 Dec 2013\n",
    "\n",
    "https://arxiv.org/abs/1312.5602v1\n",
    "\n",
    "# Abstract\n",
    "\n",
    "We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.\n",
    "\n",
    "第一个成功通过深度模型和强化学习模型直接从高维输入成功学习控制策略的模型。是一个convnet，使用Q-learning的变种来训练，输入是原始的像素，输出是评价未来奖励的价值函数。我们在Arcade学习环境中把这个方法应用到7个Atari 2600游戏上。我们发现这个方法在6个都超过之前的方法，其中的3个优于人类专家的水平。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Introduction\n",
    "\n",
    "Learning to control agents directly from high-dimensional sensory inputs like vision and speech is one of the long-standing challenges of reinforcement learning (RL). Most successful RL applications that operate on these domains have relied on hand-crafted features combined with linear value functions or policy representations. Clearly, the performance of such systems heavily relies on the quality of the feature representation.\n",
    "\n",
    "直接从高维的感官输入例如视觉和语音，来学习控制代理，一直是强化学习长期以来的挑战。在这个领域最成功的RL应用依赖于手工提取的特征和线性价值函数或者策略表达。明显的，这些系统的极大的依赖于特征表达的质量。\n",
    "\n",
    "Recent advances in deep learning have made it possible to extract high-level features from raw sensory data, leading to breakthroughs in computer vision [11, 22, 16] and speech recognition [6, 7]. These methods utilise a range of neural network architectures, including convolutional networks, multilayer perceptrons, restricted Boltzmann machines and recurrent neural networks, and have exploited both supervised and unsupervised learning. It seems natural to ask whether similar techniques could also be beneficial for RL with sensory data.\n",
    "\n",
    "最近在深度学习中的改进使得从原始感官数据中提取高层特征成为可能，这导致在机器视觉和语音识别上的突破性进展。这些方法利用了一系列的神经网络结构，包括convnet，多层感知机，受限波尔兹曼机和rnn，利用了监督和非监督学习。很自然就会要问，类似的技术是否能对使用感观数据的RL带来好处。\n",
    "\n",
    "However reinforcement learning presents several challenges from a deep learning perspective. Firstly, most successful deep learning applications to date have required large amounts of handlabelled training data. RL algorithms, on the other hand, must be able to learn from a scalar reward signal that is frequently sparse, noisy and delayed. The delay between actions and resulting rewards, which can be thousands of timesteps long, seems particularly daunting when compared to the direct association between inputs and targets found in supervised learning. Another issue is that most deep learning algorithms assume the data samples to be independent, while in reinforcement learning one typically encounters sequences of highly correlated states. Furthermore, in RL the data distribution changes as the algorithm learns new behaviours, which can be problematic for deep learning methods that assume a fixed underlying distribution.\n",
    "\n",
    "然而，从深度学习来看，强化学习提出了几个挑战。首先，目前最成功的深度学习应用需要大量人工标注的数据集。另外，RL算法学习所需要的回报标量信号往往是稀疏，嘈杂和延时的。在动作和结果回报之间的延时，可能相差上千个时间不长，相比于监督学习中输入和目标之间直接的关联，这看上去是很困难的。另一个问题是，大多数深度学习算法假设数据采样都是独立的，但在RL中，经常遇到状态高度相关的序列。再说，在RL中，当算法学习到了新的行为，数据分布就变化了，这对于假设一个固定分布的深度学习方法来说是有问题的。\n",
    "\n",
    "This paper demonstrates that a convolutional neural network can overcome these challenges to learn successful control policies from raw video data in complex RL environments. The network is trained with a variant of the Q-learning [26] algorithm, with stochastic gradient descent to update the weights. To alleviate the problems of correlated data and non-stationary distributions, we use an experience replay mechanism [13] which randomly samples previous transitions, and thereby smooths the training distribution over many past behaviors.\n",
    "\n",
    "本文显示convnet可以克服这些挑战，在一个复杂的RL环境中从原始的视频数据中成功学习控制策略。网络是通过一个Q学习算法的变种来训练的，通过SGD来更新权重。为了减轻相关性数据和非静态分布的问题，我们是了一种经验重放机制，来随机采样之前的变换，从而在很多过去的行为上平滑训练分布。\n",
    "\n",
    "We apply our approach to a range of Atari 2600 games implemented in The Arcade Learning Environment (ALE) [3]. Atari 2600 is a challenging RL testbed that presents agents with a high dimensional visual input (210 × 160 RGB video at 60Hz) and a diverse and interesting set of tasks that were designed to be difficult for humans players. Our goal is to create a single neural network agent that is able to successfully learn to play as many of the games as possible. The network was not provided with any game-specific information or hand-designed visual features, and was not privy to the internal state of the emulator; it learned from nothing but the video input, the reward and terminal signals, and the set of possible actions—just as a human player would. Furthermore the network architecture and all hyperparameters used for training were kept constant across the games. So far the network has outperformed all previous RL algorithms on six of the seven games we have attempted and surpassed an expert human player on three of them. Figure 1 provides sample screenshots from five of the games used for training.\n",
    "\n",
    "我们在Arcade学习环境中把这种方法应用到一系列Atari2600游戏。Atari2600是一个有挑战的RL测试环境，给代理一个高维的视觉输入（210x160RGB视频60Hz）和多种多样的有趣的任务集合，被设计得对于人类玩家来说困难。我们的目标是创造一个神经网络代理，它能够成功学习尽可能多的游戏。不提供任何游戏相关的信息或者手工设计的视觉特征给这个代理，并且对于模拟器的内部状态也不可知；它只能通过视频输入，回报和终止信号，还有可能的动作集合来学习，就像一个人类玩家一样。此外，网络架构和所有训练的超参数对于所有的游戏都保持一致。到目前这个网络在7个游戏中的6个已经超越了以往的RL算法，3个超越了人类玩家的水平。图1显示了5个用来训练的游戏截图。\n",
    "\n",
    "![atari_fig1.png](atari_fig1.png)\n",
    "\n",
    "Figure1: ScreenshotsfromfiveAtari2600Games:(Left-to-right)Pong,Breakout,SpaceInvaders, Seaquest, Beam Rider\n",
    "\n",
    "5个游戏，从左到右：Pong（弹球）Breakout（突围）SpaceInvaders（太空入侵者，小蜜蜂）Seaquest（深海巡弋）BeamRider（激光导弹）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Background\n",
    "\n",
    "We consider tasks in which an agent interacts with an environment E, in this case the Atari emulator, in a sequence of actions, observations and rewards. At each time-step the agent selects an action at from the set of legal game actions, A = {1, . . . , K }. The action is passed to the emulator and modifies its internal state and the game score. In general E may be stochastic. The emulator’s internal state is not observed by the agent; instead it observes an image $x_t \\in \\mathbb{R}^d$ from the emulator, which is a vector of raw pixel values representing the current screen. In addition it receives a reward $r_t$ representing the change in game score. Note that in general the game score may depend on the whole prior sequence of actions and observations; feedback about an action may only be received after many thousands of time-steps have elapsed.\n",
    "\n",
    "我们考虑一些任务。这些任务中一个代理和环境E进行交互，在Atari模拟器这种情况下，在一系列动作、观察和回报中。在每一个时间步长上，代理从一个合法的动作集合中选择一个动作，A = {1, ... ,K}。这个动作传递给模拟器，改变它的内部状态和游戏得分。一般来说，E可能是随机的。模拟器的内部状态不能被代理所观察到；而是从模拟器观察到一个图片$x_t$，是一个表示当前屏幕的原始像素值的向量。另外，它还接收一个回报$r_t$表示游戏得分的变化。注意一般来说，游戏得分依赖于整个之前的动作和观察序列；关于一个动作的反馈可能在几千步之后才能收到。\n",
    "\n",
    "Since the agent only observes images of the current screen, the task is partially observed and many emulator states are perceptually aliased, i.e. it is impossible to fully understand the current situation from only the current screen $x_t$. We therefore consider sequences of actions and observations, $s_t = x_1, a_1, x_2, ..., a_{t−1}, x_t,$ and learn game strategies that depend upon these sequences. All sequences in the emulator are assumed to terminate in a finite number of time-steps. This formalism gives rise to a large but finite Markov decision process (MDP) in which each sequence is a distinct state. As a result, we can apply standard reinforcement learning methods for MDPs, simply by using the complete sequence $s_t$ as the state representation at time t.\n",
    "\n",
    "因为代码只能观察到当前屏幕的截图，这个任务是部分观察的，也就是说，不可能仅通过当前屏幕$x_t$来完全理解当前的状态。因此，我们考虑通过动作和观察的序列$s_t$来学习游戏策略。模拟器中的所有序列都假设在有限步骤内会结束。这种形式体系产生了一个很大但是有限状态的马尔可夫决策过程（MDP），这其中每个序列都是一个不同的状态。因此，我们可以应用标准的MDP的RL方法，简单的把完整的序列$s_t$表示为在时间t上的状态。\n",
    "\n",
    "The goal of the agent is to interact with the emulator by selecting actions in a way that maximises future rewards. We make the standard assumption that future rewards are discounted by a factor of $\\gamma$ per time-step, and define the future discounted return at time t as $R_t = \\sum_{t′=t}^T \\gamma^{t'-t} r_{t'}$ , where T is the time-step at which the game terminates. We define the optimal action-value function $Q^∗ (s, a)$ as the maximum expected return achievable by following any strategy, after seeing some sequence s and then taking some action a, $Q^∗(s, a) = max_{\\pi} \\mathbb{E} [R_t|s_t = s, a_t = a, \\pi]$, where $\\pi$ is a policy mapping sequences to actions (or distributions over actions).\n",
    "\n",
    "代理的目标是和模拟器进行交互，通过选择动作来最大化未来的回报。我们做了标准的假设，未来的回报是在每个时间步骤上进行因子为$\\gamma$的折扣，定义在时间t的未来折扣回报$R_t$，T是游戏终止时的时间。定义最优的动作-价值函数$Q^*$ 是遵循任何策略，看见某种序列s，然后执行某个动作a，所获得的最大回报期望，$\\pi$是一个通过序列到动作的政策映射（或者动作的分布）。\n",
    "\n",
    "The optimal action-value function obeys an important identity known as the Bellman equation. This is based on the following intuition: if the optimal value $Q^∗(s′,a′)$ of the sequence s′ at the next time-step was known for all possible actions a′, then the optimal strategy is to select the action a′ maximising the expected value of r + γQ∗(s′, a′),\n",
    "\n",
    "最优的动作-价值函数遵守一个重要的恒等式，叫做贝尔曼方程。基于以下直觉，如果序列s'在下一步所有的动作a'的最优的价值$Q^*$已知，最优策略是选择动作a'最大化期望值，\n",
    "\n",
    "$$\n",
    "Q^*(s,a) = \\mathbb{E}_{s′ \\sim \\epsilon } [r + \\gamma max_{a'} Q^*(s', a') | s, a]\n",
    "$$\n",
    "\n",
    "The basic idea behind many reinforcement learning algorithms is to estimate the action-value function, by using the Bellman equation as an iterative update, $Q_{i+1}(s,a) = E [r + γ max_{a′} Qi (s′ , a′ )|s, a]$. Such value iteration algorithms converge to the optimal action-value function, $Q_i \\rightarrow Q^∗$ as $i \\rightarrow \\infty $ [23]. In practice, this basic approach is totally impractical, because the action-value function is estimated separately for each sequence, without any generalisation. Instead, it is common to use a function approximator to estimate the action-value function, Q(s, a; θ) ≈ Q∗(s, a). In the reinforcement learning community this is typically a linear function approximator, but sometimes a non-linear function approximator is used instead, such as a neural network. We refer to a neural network function approximator with weights θ as a Q-network. A Q-network can be trained by minimising a sequence of loss functions Li(θi) that changes at each iteration i,\n",
    "\n",
    "许多RL算法背后的基本想法都是估计动作-价值函数，通过bellman公式作为交互式更新，这种价值迭代算法当i趋近于无穷的时候，价值函数收敛到最优解。实际上，这种基本的方法完全不切实际，因为动作-价值函数对每个步骤是单独估计的，没有任何泛化。相反，通过函数逼近来估计动作-价值函数非常常见。在RL社区，这通常是一个线性函数逼近，但是有时候也会用非线性函数，例如神经网络。我们称以权重$\\theta$为参数的一个神经网络为Q-net网络。一个Q-net可以最小化$L_i$损失函数来训练，\n",
    "\n",
    "$$\n",
    "L_i (\\theta_i) = \\mathbb{E}_{s, a\\sim \\rho(·)} [ (y_i − Q (s,a; \\theta_i))^2 ]\n",
    "$$\n",
    "\n",
    "where yi = Es′∼E [r + γ maxa′ Q(s′, a′; θi−1)|s, a] is the target for iteration i and ρ(s, a) is a probability distribution over sequences s and actions a that we refer to as the behaviour distribution. The parameters from the previous iteration θi−1 are held fixed when optimising the loss function Li (θi). Note that the targets depend on the network weights; this is in contrast with the targets used for supervised learning, which are fixed before learning begins. Differentiating the loss function with respect to the weights we arrive at the following gradient,\n",
    "\n",
    "$$\n",
    "∇θiLi(θi)=Es,a∼ρ(·);s′∼E r+γmaxQ(s,a;θi−1)−Q(s,a;θi) ∇θiQ(s,a;θi)\n",
    "$$\n",
    "\n",
    "Rather than computing the full expectations in the above gradient, it is often computationally expedient to optimise the loss function by stochastic gradient descent. If the weights are updated after every time-step, and the expectations are replaced by single samples from the behaviour distribution ρ and the emulator E respectively, then we arrive at the familiar Q-learning algorithm [26].\n",
    "\n",
    "Note that this algorithm is model-free: it solves the reinforcement learning task directly using samples from the emulator E, without explicitly constructing an estimate of E. It is also off-policy: it learns about the greedy strategy a = maxa Q(s, a; θ), while following a behaviour distribution that ensures adequate exploration of the state space. In practice, the behaviour distribution is often selected by an ε-greedy strategy that follows the greedy strategy with probability 1 − ε and selects a random action with probability ε.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Deep Reinforcement Learning\n",
    "\n",
    "Recent breakthroughs in computer vision and speech recognition have relied on efficiently training deep neural networks on very large training sets. The most successful approaches are trained directly from the raw inputs, using lightweight updates based on stochastic gradient descent. By feeding sufficient data into deep neural networks, it is often possible to learn better representations than handcrafted features [11]. These successes motivate our approach to reinforcement learning. Our goal is to connect a reinforcement learning algorithm to a deep neural network which operates directly on RGB images and efficiently process training data by using stochastic gradient updates.\n",
    "\n",
    "Tesauro’s TD-Gammon architecture provides a starting point for such an approach. This architecture updates the parameters of a network that estimates the value function, directly from on-policy samples of experience, st, at, rt, st+1, at+1, drawn from the algorithm’s interactions with the environment (or by self-play, in the case of backgammon). Since this approach was able to outperform the best human backgammon players 20 years ago, it is natural to wonder whether two decades of hardware improvements, coupled with modern deep neural network architectures and scalable RL algorithms might produce significant progress.\n",
    "\n",
    "In contrast to TD-Gammon and similar online approaches, we utilize a technique known as experience replay [13] where we store the agent’s experiences at each time-step, et = (st, at, rt, st+1) in a data-set D = e1 , ..., eN , pooled over many episodes into a replay memory. During the inner loop of the algorithm, we apply Q-learning updates, or minibatch updates, to samples of experience, e ∼ D, drawn at random from the pool of stored samples. After performing experience replay, the agent selects and executes an action according to an ε-greedy policy. Since using histories of arbitrary length as inputs to a neural network can be difficult, our Q-function instead works on fixed length representation of histories produced by a function φ. The full algorithm, which we call deep Q-learning, is presented in Algorithm 1.\n",
    "\n",
    "This approach has several advantages over standard online Q-learning [23]. First, each step of experience is potentially used in many weight updates, which allows for greater data efficiency."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
