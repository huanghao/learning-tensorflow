{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One by One [ 1 x 1 ] Convolution - counter-intuitively useful\n",
    "----\n",
    "\n",
    "http://iamaaditya.github.io/2016/03/one-by-one-convolution/\n",
    "\n",
    "https://github.com/vdumoulin/conv_arithmetic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "left: Convolution with kernel of size 3x3| right: Convolution with kernel of size 1x1\n",
    "- | -\n",
    "![Conv3x3](https://raw.githubusercontent.com/iamaaditya/iamaaditya.github.io/master/images/conv_arithmetic/full_padding_no_strides_transposed.gif) | ![Conv1x1](https://raw.githubusercontent.com/iamaaditya/iamaaditya.github.io/master/images/conv_arithmetic/full_padding_no_strides_transposed_small.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Answer\n",
    "\n",
    "Most simplistic explanation would be that 1x1 convolution leads to dimension reductionality. For example, an image of 200 x 200 with 50 features on convolution with 20 filters of 1x1 would result in size of 200 x 200 x 20. But then again, is this is the best way to do dimensionality reduction in the convoluational neural network? What about the efficacy vs efficiency?\n",
    "\n",
    "最简单的解释是1x1的卷积可以减低维度。例如，一个200 x 200 x 50的卷积特征图片，使用20个1x1卷积会得到200 x 200 x 20的结果。但是这种降维的方法在cnn来说是最有效的一种方式吗？对于性能和效率来说呢？\n",
    "\n",
    "# Complex Answer\n",
    "\n",
    "## Feature transformation\n",
    "\n",
    "Although 1x1 convolution is a ‘feature pooling’ technique, there is more to it than just sum pooling of features across various channels/feature-maps of a given layer. 1x1 convolution acts like coordinate-dependent transformation in the filter space[1]. It is important to note here that this transformation is strictly linear, but in most of application of 1x1 convolution, it is succeeded by a non-linear activation layer like ReLU. This transformation is learned through the (stochastic) gradient descent. But an important distinction is that it suffers with less over-fitting due to smaller kernel size (1x1).\n",
    "\n",
    "尽管1x1卷积是一种“特征池化”技术，但是它不仅仅只是把一层中的多个通道的属性加和。1x1卷积就像在filter的空间做了coordinate-dependent的变换。要特别强调的是这种变化是线性的，但在大多数情况下1x1卷积后会跟一个非线性激励层，例如relu。这种变换是由梯度下降来学习的。一个很重要的区别是对于较小的卷积核（1x1）更不容易受到过拟合的影响。\n",
    "\n",
    "## Deeper Network\n",
    "\n",
    "One by One convolution was first introduced in this paper titled Network in Network. In this paper, the author’s goal was to generate a deeper network without simply stacking more layers. It replaces few filters with a smaller perceptron layer with mixture of 1x1 and 3x3 convolutions. In a way, it can be seen as “going wide” instead of “deep”, but it should be noted that in machine learning terminology, ‘going wide’ is often meant as adding more data to the training. Combination of 1x1 (x F) convolution is mathematically equivalent to a multi-layer perceptron.[2].\n",
    "\n",
    "1x1卷积最初是由Network in Network提出的。在这篇论文中，作者的目标是为了生成一个更深的网络，但不想通过简单地叠加更多的层。它把少量的滤波器换成了较小的1x1和3x3的感知层。从某种程度上来说，这可以被看成“变宽”而不是“变深”。但是需要说明的是，在机器学习的专业术语里，“变宽”经常表示增加更多的训练数据。数学上，使用1x1卷积等同于多层感知机。\n",
    "\n",
    "### Inception Module\n",
    "\n",
    "In GoogLeNet architecture, 1x1 convolution is used for two purposes\n",
    "\n",
    "- To make network deep by adding an “inception module” like Network in Network paper, as described above.\n",
    "- To reduce the dimensions inside this “inception module”.\n",
    "- To add more non-linearity by having ReLU immediately after every 1x1 convolution.\n",
    "\n",
    "Here is the scresnshot from the paper, which elucidates above points :\n",
    "\n",
    "在GooLeNet中，1x1卷积有两个作用：\n",
    "\n",
    "- 通过添加“inception module”使网络变深，就像Network in Network里的那样\n",
    "- 在inception module中减少维度\n",
    "- 通过在1x1卷积和紧跟的relu增加更多的非线性\n",
    "\n",
    "![inception](https://raw.githubusercontent.com/iamaaditya/iamaaditya.github.io/master/images/inception_1x1.png) \n",
    "\n",
    "It can be seen from the image on the right, that 1x1 convolutions (in yellow), are specially used before 3x3 and 5x5 convolution to reduce the dimensions. It should be noted that a two step convolution operation can always to combined into one, but in this case and in most other deep learning networks, convolutions are followed by non-linear activation and hence convolutions are no longer linear operators and cannot be combined.\n",
    "\n",
    "从右边图中可以看出，1x1卷积（黄色）被特意加到了3x3和5x5卷积之前，用来降维。需要指出的是，两个卷积操作总是可以被合并成一个。但是这这里和其他很多的深度网络中，卷积层的后面会紧跟一个非线性激励，所以卷积层就不再是线性操作，而且就不能被合并了。\n",
    "\n",
    "In designing such a network, it is important to note that initial convolution kernel should be of size larger than 1x1 to have a receptive field capable of capturing locally spatial information. According to the NIN paper, 1x1 convolution is equivalent to cross-channel parametric pooling layer. From the paper - “This cascaded cross channel parameteric pooling structure allows complex and learnable interactions of cross channel information”.\n",
    "\n",
    "在设计这类网络的时候，很重要的一点是，最开始的卷积核大小需要大于1x1，这样才能使感知域能够捕获空间上的局域信息。按照NIN论文的说法，1x1卷积等同于跨通道的参数池化层。原文指出“这个叠加的跨通道参数池化结构可以在不同通道的信息上产生复杂的可学习的交互。”\n",
    "\n",
    "Cross channel information learning (cascaded 1x1 convolution) is biologically inspired because human visual cortex have receptive fields (kernels) tuned to different orientation. For e.g\n",
    "\n",
    "跨通道信息学习（叠加1x1卷积）来源于生物学的灵感。人的视觉脑皮层中存在不同方向的感知域（卷积核）。例如：\n",
    "\n",
    "![cortex](https://raw.githubusercontent.com/iamaaditya/iamaaditya.github.io/master/images/conv_arithmetic/RotBundleFiltersListPlot3D.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Uses\n",
    "\n",
    "- 1x1 Convolution can be combined with Max pooling\n",
    "\n",
    "![1x1-maxpool](https://raw.githubusercontent.com/iamaaditya/iamaaditya.github.io/master/images/conv_arithmetic/numerical_max_pooling.gif)\n",
    "\n",
    "- 1x1 Convolution with higher strides leads to even more redution in data by decreasing resolution, while losing very little non-spatially correlated information.\n",
    "\n",
    "![1x1-strides](https://raw.githubusercontent.com/iamaaditya/iamaaditya.github.io/master/images/conv_arithmetic/no_padding_strides.gif)\n",
    "\n",
    "- Replace fully connected layers with 1x1 convolutions as Yann LeCun believes they are the same -\n",
    "\n",
    "    In Convolutional Nets, there is no such thing as “fully-connected layers”. There are only convolution layers with 1x1 convolution kernels and a full connection table. – Yann LeCun \n",
    "\n",
    "    在卷积网络中，没有“全连接层”这种说法。只有1x1卷积核和全连接表构成的卷积层。\n",
    "    \n",
    "Convolution gif images generated using this wonderful code, more images on 1x1 convolutions and 3x3 convolutions can be found here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
