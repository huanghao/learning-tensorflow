{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FrozenLake-v0\n",
    "====\n",
    "\n",
    "![frozenlake.jpg](frozenlake.jpg)\n",
    "\n",
    "https://github.com/yandexdataschool/Practical_RL/blob/master/week0/frozenlake.ipynb\n",
    "\n",
    "https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0\n",
    "\n",
    "https://gym.openai.com/evaluations/eval_BuxTzFMwTfKQr2mCwos1uA\n",
    "\n",
    "\n",
    "这些代码用的都是基于表格的Q函数。使用了类似于epsilon greedy的方法来产生随机动作。得分大概在30-80之间，很难再高。\n",
    "\n",
    "    SFFF\n",
    "    FHFH\n",
    "    FFFH\n",
    "    HFFG\n",
    "\n",
    "这个布局是固定的。S在左上角，G在右下角。H表示hole，F表示frozen。目标是要从S到G，只能经过F，而不能掉到H里。能采取的动作就是上下左右，但这个动作得到的结果有一定的随机性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-21 13:44:23,562] Making new env: FrozenLake-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "n_states: 16 n_actions: 4 max_steps: 100\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "env = gym.make('FrozenLake-v0')\n",
    "env.render()\n",
    "print()\n",
    "\n",
    "# 关于这个env的一些信息\n",
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "max_steps = env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')\n",
    "print('n_states:', n_states, 'n_actions:', n_actions, 'max_steps:', max_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate\n",
    "\n",
    "\n",
    "Q也叫action-value function。输入状态和动作，得到价值的期望。Q(state, action) -> v\n",
    "\n",
    "先写个函数用来评价不同Q的好坏。运行100次episode，计算reward的平均值，越高表示这个Q越好。\n",
    "\n",
    "上面的tutorial里有评价policy的代码。稍微调整一下就行。policy(state) -> action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_reward(env, policy, t_max=100):\n",
    "    \"\"\"\n",
    "    Interact with an environment, return sum of all rewards.\n",
    "    If game doesn't end on t_max (e.g. agent walks into a wall),\n",
    "    force end the game and return whatever reward you got so far.\n",
    "    Tip: see signature of env.step(...) method above.\n",
    "    \"\"\"\n",
    "    s = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    for _ in range(t_max):\n",
    "        s, r, is_done, _ = env.step(policy[s])\n",
    "        total_reward += r\n",
    "        if is_done:\n",
    "            break\n",
    "\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def evaluate(env, policy, n_times=100):\n",
    "    \"\"\"Run several evaluations and average the score the policy gets.\"\"\"\n",
    "    rewards = [sample_reward(env, policy) for _ in range(n_times)]\n",
    "    return float(np.mean(rewards))\n",
    "\n",
    "\n",
    "def q_to_policy(env, q):\n",
    "    p = {}\n",
    "    for s in range(env.observation_space.n):\n",
    "        p[s] = np.argmax(q[s])\n",
    "    return p\n",
    "\n",
    "\n",
    "def evaluate_q(env, q, n_times=100):\n",
    "    return evaluate(env, q_to_policy(env, q), n_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate a random q function\n",
    "\n",
    "def random_q():\n",
    "    return np.random.random((n_states, n_actions))\n",
    "\n",
    "evaluate_q(env, random_q())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 709.52it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train(env, n=500, lr=.81, gamma=.96, steps=100):\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "    for i in tqdm(range(n)):\n",
    "        s = env.reset()\n",
    "        for _ in range(steps):\n",
    "            # 在value上加了一个随机量，这个随机量越来越小 (noise greedy)\n",
    "            a = np.argmax(Q[s,:] + np.random.randn(env.action_space.n) * (1. / (i+1)))\n",
    "            s1, r, d, _ = env.step(a)\n",
    "            qsa = Q[s, a]\n",
    "            Q[s, a] += lr * (r + gamma * np.max(Q[s1,:]) - qsa)\n",
    "\n",
    "            if d:\n",
    "                break\n",
    "            s = s1\n",
    "\n",
    "    return Q\n",
    "\n",
    "evaluate_q(env, train(env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 1298.34it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.39"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用这种形式的 epsilon greedy 得到的结果较差的情况比较多\n",
    "\n",
    "def train(env, n=500, lr=.81, gamma=.96, steps=100, epsilon=1., epsilon_decay=.98):\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "    for i in tqdm(range(n)):\n",
    "        s = env.reset()\n",
    "        for _ in range(steps):\n",
    "            if np.random.random() < epsilon:\n",
    "                a = env.action_space.sample()\n",
    "            else:\n",
    "                a = np.argmax(Q[s, :])\n",
    "            s1, r, d, _ = env.step(a)\n",
    "            qsa = Q[s, a]\n",
    "            Q[s, a] += lr * (r + gamma * np.max(Q[s1,:]) - qsa)\n",
    "\n",
    "            if d:\n",
    "                break\n",
    "            s = s1\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "    return Q\n",
    "\n",
    "evaluate_q(env, train(env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
